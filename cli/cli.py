#!/usr/bin/env python3
"""
CLI interactive pour le traitement de fichiers JSON
avec options pour l'analyse automatique, la d√©tection de type,
et l'int√©gration optionnelle avec un LLM.
"""

import os
import sys
import json
import glob
import dotenv
import time
import shutil
import subprocess
from datetime import datetime
from pathlib import Path
from typing import List, Optional, Dict, Any, Tuple

import typer
from rich.console import Console
from rich.progress import Progress, SpinnerColumn, TextColumn, BarColumn, TimeElapsedColumn
from rich.table import Table
from rich.panel import Panel
from rich.syntax import Syntax
import inquirer
from openai import OpenAI
from rich.markdown import Markdown
from rich.rule import Rule
from extract.image_describer import PDFImageDescriber
import rich

# Importer le module de gestion des langues
try:
    from cli.lang_utils import t, set_language, get_current_language
    TRANSLATIONS_LOADED = True
except ImportError:
    # Fallback si le module de traduction n'est pas disponible
    def t(key, category=None, lang=None):
        return key
    def set_language(lang):
        pass
    def get_current_language():
        return "fr"
    TRANSLATIONS_LOADED = False

# Charger les variables d'environnement
dotenv.load_dotenv()

# Ajout du r√©pertoire parent au chemin de recherche Python
current_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir = os.path.dirname(current_dir)
if parent_dir not in sys.path:
    sys.path.insert(0, parent_dir)

# Import des modules personnalis√©s du projet
try:
    from extract.generic_json_processor import GenericJsonProcessor
    from extract.extract_jira_structure import extract_structure_from_first_object as extract_jira_structure
    from extract.extract_confluence_structure import extract_structure_from_first_object as extract_confluence_structure
    from extract.match_jira_confluence import find_matches, update_with_matches
    from extract.robust_json_parser import robust_json_parser, JsonParsingException
except ImportError as e:
    print(f"Erreur d'importation: {e}")
    print("V√©rifiez que tous les modules requis sont install√©s et que la structure du projet est correcte.")
    sys.exit(1)

# Import du nouveau module Outlines (avec gestion d'erreur si non install√©)
try:
    from extract.outlines_enhanced_parser import outlines_robust_json_parser
    from extract.outlines_extractor import process_jira_data, process_confluence_data
    OUTLINES_AVAILABLE = True
except ImportError:
    print("Module Outlines non disponible. Fonctionnalit√©s d'extraction avanc√©es d√©sactiv√©es.")
    OUTLINES_AVAILABLE = False

# Ajouter l'import pour le r√©sum√© LLM
try:
    sys.path.append(os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), "extract"))
    from extract.llm_summary import generate_llm_summary
except ImportError:
    try:
        from llm_summary import generate_llm_summary
    except ImportError:
        generate_llm_summary = None
        print("‚ö†Ô∏è Module llm_summary non trouv√©, la g√©n√©ration de r√©sum√©s LLM est d√©sactiv√©e")

# Initialisation de Typer et Rich
app = typer.Typer(help="JSON Processor pour Llamendex")
console = Console()

# Constantes
DEFAULT_MAPPINGS_DIR = os.environ.get("MAPPINGS_DIR", "extract/mapping_examples")
DEFAULT_OUTPUT_DIR = os.environ.get("OUTPUT_DIR", "output")
LLM_MODELS = os.environ.get("LLM_MODELS", "gpt-4.1,gpt-3.5-turbo,o3,gpt-4").split(",")
DEFAULT_LLM_MODEL = os.environ.get("DEFAULT_LLM_MODEL", "gpt-4.1")

# --- HEADER/LOGO ---
def print_header():
    logo = (
        "[bold magenta]"
        "BBBBB   L       AAAAA   III  K   K  EEEEE\n"
        "B    B  L      A     A  I   K  K   E\n"
        "BBBBB   L      AAAAAAA  I   KKK    EEEE\n"
        "B    B  L      A     A  I   K  K   E\n"
        "BBBBB   LLLLL  A     A III  K   K  EEEEE\n"
        "[/bold magenta]"
    )
    console.print(logo)
    console.print("[bold cyan]D A T A F L O W    A I    C L I[/bold cyan] [green]v1.0[/green] üöÄ\n")
    
    # Afficher la langue actuelle
    if TRANSLATIONS_LOADED:
        current_lang = get_current_language()
        lang_display = "Fran√ßais" if current_lang == "fr" else "English"
        console.print(f"[dim]Language/Langue: {lang_display}[/dim]\n")

# --- STEPPER ---
def print_stepper(current:int, total:int, steps:list):
    stepper = ""
    for i, step in enumerate(steps, 1):
        if i < current:
            stepper += f"[green]‚óè[/green] "
        elif i == current:
            stepper += f"[bold yellow]‚û§ {step}[/bold yellow] "
        else:
            stepper += f"[grey]‚óã {step}[/grey] "
        if i < total:
            stepper += "[grey]‚Üí[/grey] "
    console.print(stepper)

# --- NAVIGATION AVEC ICONES ---
def _prompt_for_file(message: str, allow_validate: bool = False, file_extension: str = ".json") -> Optional[str]:
    """Assure que le s√©lecteur de fichiers commence toujours dans le dossier 'files' du projet."""
    # Trouver le chemin du dossier files √† la racine du projet
    files_dir = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), "files")
    
    # V√©rifier si le dossier files existe, sinon utiliser le r√©pertoire courant
    if os.path.exists(files_dir):
        current_dir = files_dir
    else:
        current_dir = os.getcwd()
        console.print(f"[yellow]Dossier 'files' non trouv√©. Utilisation du r√©pertoire courant.[/yellow]")
    
    while True:
        console.print(f"\n[bold]{t('current_directory', 'messages')}[/bold] {current_dir}")
        items = os.listdir(current_dir)
        files = [f for f in items if os.path.isfile(os.path.join(current_dir, f)) and f.endswith(file_extension)]
        dirs = [d for d in items if os.path.isdir(os.path.join(current_dir, d))]
        
        # Trier les dossiers et les fichiers
        dirs.sort()
        files.sort()
        
        choices = []
        
        # Option pour remonter au dossier parent
        if current_dir != os.path.dirname(current_dir):
            choices.append(t("parent_dir", "options"))
        
        # Ajouter les dossiers
        choices += [f"{t('dir_prefix', 'options')} {d}" for d in dirs]
        
        # Ajouter les fichiers filtr√©s
        choices += [f"{t('file_prefix', 'options')} {f}" for f in files]
        
        # Ajouter les options de validation/annulation
        if allow_validate:
            choices.append(t("validate", "options"))
        choices.append(t("enter_manually", "options"))
        choices.append(t("cancel", "options"))
        
        # Poser la question
        questions = [
            inquirer.List('path',
                         message=message,
                         choices=choices,
                         carousel=True)
        ]
        
        try:
            answers = inquirer.prompt(questions)
            if answers is None or not answers['path']:
                return None
                
            choice = answers['path']
            
            # Traiter le choix
            if choice == t("cancel", "options"):
                console.print(f"[italic]{t('operation_cancelled', 'messages')}[/italic]")
                return None
                
            elif choice == t("parent_dir", "options"):
                current_dir = os.path.dirname(current_dir)
                os.chdir(current_dir)
                
            elif choice == t("enter_manually", "options"):
                # Demander un chemin manuellement
                manual_path = inquirer.prompt([
                    inquirer.Text('path',
                                 message=t("manual_path", "prompts"))
                ])
                
                if manual_path and manual_path['path']:
                    path = os.path.expanduser(manual_path['path'])
                    if os.path.exists(path) and (os.path.isdir(path) or (os.path.isfile(path) and path.endswith(file_extension))):
                        if os.path.isdir(path):
                            current_dir = path
                            os.chdir(current_dir)
                        else:
                            return path
                    else:
                        console.print(f"[bold red]{t('invalid_path', 'messages')}[/bold red]")
                        
            elif choice == t("validate", "options"):
                return "__VALIDATE__"
                
            elif choice.startswith(t("dir_prefix", "options").split(' ')[0]):
                # Changer de dossier
                folder = choice.split(' ', 2)[-1]
                new_dir = os.path.join(current_dir, folder)
                if os.path.isdir(new_dir):
                    current_dir = new_dir
                    os.chdir(current_dir)
                    
            elif choice.startswith(t("file_prefix", "options").split(' ')[0]):
                # S√©lectionner un fichier
                file = choice.split(' ', 2)[-1]
                file_path = os.path.join(current_dir, file)
                if os.path.isfile(file_path) and file_path.endswith(file_extension):
                    return file_path
                else:
                    console.print(f"[bold red]{t('invalid_path', 'messages')}[/bold red]")
        except Exception as e:
            console.print(f"[bold red]Erreur lors de la s√©lection du fichier: {e}[/bold red]")
            return None

# --- FEEDBACK/RESUME ---
def print_success(msg:str):
    console.print(f"[bold green]‚úÖ {msg}[/bold green]")
def print_error(msg:str):
    console.print(f"[bold red]‚ùå {msg}[/bold red]")
def print_info(msg:str):
    console.print(f"[bold blue]‚ÑπÔ∏è  {msg}[/bold blue]")
def print_warning(msg:str):
    console.print(f"[bold yellow]‚ö†Ô∏è  {msg}[/bold yellow]")

def ensure_dir(directory: str):
    """Assure que le r√©pertoire existe, le cr√©e si n√©cessaire."""
    Path(directory).mkdir(parents=True, exist_ok=True)

def find_mapping_files() -> List[str]:
    """Trouve tous les fichiers de mapping disponibles dans le r√©pertoire par d√©faut."""
    mapping_pattern = os.path.join(DEFAULT_MAPPINGS_DIR, "*.json")
    return [os.path.basename(f) for f in glob.glob(mapping_pattern)]

def detect_file_type(file_path: str) -> dict:
    """
    D√©tecte le type de fichier JSON (JIRA ou Confluence)
    """
    try:
        # Extraire la structure JIRA
        jira_structure = extract_jira_structure(file_path)
        
        # V√©rifier si c'est un fichier JIRA
        if not jira_structure.get("error") and jira_structure.get("structure"):
            keys = jira_structure["structure"].get("keys", [])
            if any(key in keys for key in ["key", "summary", "issuetype", "status"]):
                return {"type": "jira", "structure": jira_structure["structure"]}
        
        # Extraire la structure Confluence
        confluence_structure = extract_confluence_structure(file_path)
        
        # V√©rifier si c'est un fichier Confluence
        if not confluence_structure.get("error") and confluence_structure.get("structure"):
            keys = confluence_structure["structure"].get("keys", [])
            if any(key in keys for key in ["title", "space", "body", "content"]):
                return {"type": "confluence", "structure": confluence_structure["structure"]}
        
        # Type inconnu
        return {"type": "unknown", "message": "Type de fichier non reconnu"}
    
    except Exception as e:
        return {"type": "error", "message": str(e)}

def process_with_llm(content: Dict[str, Any], model: str = None, api_key: str = None) -> Dict[str, Any]:
    """
    Enrichit les donn√©es avec l'analyse LLM.
    
    Args:
        content: Contenu JSON d√©j√† trait√©
        model: Mod√®le LLM √† utiliser
        api_key: Cl√© API pour le LLM
        
    Returns:
        Contenu enrichi avec l'analyse LLM
    """
    try:
        # Utiliser les variables d'environnement si non sp√©cifi√©es
        api_key = api_key or os.environ.get("OPENAI_API_KEY")
        model = model or os.environ.get("DEFAULT_LLM_MODEL", "gpt-4.1")
        
        if not api_key:
            console.print("[bold red]Erreur: Cl√© API OpenAI manquante. D√©finissez la variable d'environnement OPENAI_API_KEY ou utilisez l'option --api-key.[/bold red]")
            return content
            
        client = OpenAI(api_key=api_key)
        
        # Si contenu contient des √©l√©ments (tickets, pages, etc.)
        if "items" in content and content["items"]:
            with Progress(
                SpinnerColumn(),
                TextColumn("[bold blue]{task.description}"),
                BarColumn(),
                TimeElapsedColumn(),
            ) as progress:
                task = progress.add_task(f"[cyan]Analyse LLM avec {model}...", total=len(content["items"]))
                
                for item in content["items"]:
                    # Extraire le contenu textuel pour l'analyse
                    text_content = ""
                    if "content" in item:
                        if isinstance(item["content"], dict):
                            for k, v in item["content"].items():
                                if isinstance(v, str):
                                    text_content += v + "\n"
                        elif isinstance(item["content"], str):
                            text_content = item["content"]
                    
                    # Si nous avons du texte √† analyser
                    if text_content:
                        prompt = f"""
                        Analyse le contenu suivant et fournis :
                        1. Une liste de mots-cl√©s pertinents (max 7)
                        2. Une classification du type de contenu
                        3. Un r√©sum√© de 2-3 phrases
                        
                        Contenu √† analyser :
                        {text_content[:1500]}  # Limiter pour rester dans les limites de tokens
                        
                        Format de r√©ponse (JSON) :
                        {{
                            "keywords": ["mot1", "mot2", ...],
                            "content_type": "type de contenu",
                            "summary": "r√©sum√© concis"
                        }}
                        """
                        
                        response = client.chat.completions.create(
                            model=model,
                            messages=[
                                {"role": "system", "content": "Tu es un assistant sp√©cialis√© dans l'analyse de donn√©es textuelles."},
                                {"role": "user", "content": prompt}
                            ],
                            response_format={"type": "json_object"}
                        )
                        
                        try:
                            llm_analysis = json.loads(response.choices[0].message.content)
                            
                            # Ajouter l'analyse au contenu
                            if "analysis" not in item:
                                item["analysis"] = {}
                            
                            item["analysis"]["llm"] = llm_analysis
                            
                            # Ajouter ou fusionner les mots-cl√©s
                            if "keywords" in llm_analysis:
                                if "keywords" not in item["analysis"]:
                                    item["analysis"]["keywords"] = []
                                # √âviter les doublons
                                for kw in llm_analysis["keywords"]:
                                    if kw not in item["analysis"]["keywords"]:
                                        item["analysis"]["keywords"].append(kw)
                        except json.JSONDecodeError:
                            # Fallback si le format JSON est incorrect
                            if "analysis" not in item:
                                item["analysis"] = {}
                            item["analysis"]["llm_raw"] = response.choices[0].message.content
                    
                    progress.update(task, advance=1)
        
        return content
    
    except Exception as e:
        console.print(f"[bold red]Erreur lors de l'analyse LLM: {e}[/bold red]")
        return content

@app.command()
def process(
    input_file: str = typer.Argument(..., help="Fichier JSON √† traiter"),
    output_file: Optional[str] = typer.Option(None, "--output", "-o", help="Fichier de sortie (par d√©faut: {input}_processed.json)"),
    mapping_file: Optional[str] = typer.Option(None, "--mapping", "-m", help="Fichier de mapping √† utiliser"),
    detect: bool = typer.Option(True, "--detect/--no-detect", help="D√©tecter automatiquement le type de fichier"),
    auto_mapping: bool = typer.Option(True, "--auto-mapping/--no-auto-mapping", help="Utiliser le mapping correspondant au type d√©tect√©"),
    use_llm: bool = typer.Option(True, "--llm/--no-llm", help="Utiliser un LLM pour l'enrichissement"),
    llm_model: str = typer.Option(None, "--model", help="Mod√®le LLM √† utiliser"),
    preserve_source: bool = typer.Option(True, "--preserve-source/--overwrite-source", help="Pr√©server les fichiers sources originaux"),
):
    """Traiter un fichier JSON selon son type d√©tect√©."""
    console = Console()
    
    # 1. V√©rifier que le fichier existe
    if not os.path.exists(input_file):
        console.print(f"[bold red]Le fichier {input_file} n'existe pas.[/bold red]")
        raise typer.Exit(1)
    
    # 2. V√©rifier que le fichier est un JSON valide
    try:
        from tools import validate_file
        is_valid, error_msg = validate_file(input_file)
        if not is_valid:
            console.print(f"[bold red]Le fichier JSON n'est pas valide: {error_msg}[/bold red]")
            
            # Proposer de r√©parer le fichier
            if typer.confirm("Voulez-vous essayer de r√©parer le fichier?"):
                try:
                    from tools import repair_json_files
                    repaired_file = input_file + ".repaired"
                    if repair_json_files(input_file, repaired_file):
                        console.print(f"[bold green]Fichier r√©par√© et sauvegard√© dans {repaired_file}[/bold green]")
                        input_file = repaired_file
                    else:
                        console.print("[bold red]Impossible de r√©parer le fichier.[/bold red]")
                        raise typer.Exit(1)
                except ImportError:
                    console.print("[bold yellow]Module tools non trouv√©. Impossible de r√©parer le fichier.[/bold yellow]")
                    raise typer.Exit(1)
    except ImportError:
        console.print("[bold yellow]Module tools non trouv√©. V√©rification de validit√© ignor√©e.[/bold yellow]")
    
    # G√©n√©rer un timestamp pour le nom du fichier
    timestamp = datetime.now().strftime("%Y-%m-%d-%H-%M-%S")
    
    # Obtenir le chemin absolu vers le r√©pertoire racine du projet
    project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    
    # Utiliser le dossier results comme base √† la racine du projet
    base_results_dir = os.path.join(project_root, "results")
    os.makedirs(base_results_dir, exist_ok=True)
    
    # Si pas de fichier de sortie sp√©cifi√©, cr√©er un nom par d√©faut avec timestamp dans le dossier results
    if not output_file:
        base, ext = os.path.splitext(os.path.basename(input_file))
        output_file = os.path.join(base_results_dir, f"{base}_processed_{timestamp}{ext}")
    elif not os.path.isabs(output_file) and not output_file.startswith(base_results_dir):
        # Si le chemin n'est pas absolu et ne commence pas par le dossier results,
        # placer le fichier dans le dossier results
        output_file = os.path.join(base_results_dir, output_file)
    
    # 2. Afficher les infos de d√©part
    console.print(Panel.fit(
        "[bold]Traitement de fichier JSON pour Llamendex[/bold]\n\n"
        f"Fichier d'entr√©e : [cyan]{input_file}[/cyan]\n"
        f"Fichier de sortie : [cyan]{output_file}[/cyan]\n"
        f"Pr√©servation des sources : [cyan]{'Oui' if preserve_source else 'Non'}[/cyan]",
        title="JSON Processor",
        border_style="blue"
    ))
    
    # 3. D√©tection du type (optionnelle)
    file_type = None
    if detect:
        with console.status("[bold blue]D√©tection du type de fichier..."):
            file_type = detect_file_type(input_file)
        
        if file_type:
            console.print(f"Type d√©tect√© : [bold green]{file_type['type']}[/bold green]")
        else:
            console.print("[yellow]Type de fichier non d√©tect√©. Traitement g√©n√©rique sera utilis√©.[/yellow]")
    
    # 4. Gestion du mapping (interactif ou automatique)
    if auto_mapping and file_type and not mapping_file:
        auto_mapping_file = os.path.join(DEFAULT_MAPPINGS_DIR, f"{file_type['type']}_mapping.json")
        if os.path.exists(auto_mapping_file):
            mapping_file = auto_mapping_file
            console.print(f"Utilisation automatique du mapping : [cyan]{mapping_file}[/cyan]")
    
    # 7. Traitement principal
    try:
        # Charger le mapping si sp√©cifi√©
        field_mappings = None
        if mapping_file and os.path.exists(mapping_file):
            try:
                with open(mapping_file, 'r', encoding='utf-8') as f:
                    field_mappings = json.load(f)
                console.print(f"Mapping charg√© depuis [cyan]{mapping_file}[/cyan]")
            except Exception as e:
                console.print(f"[bold red]Erreur lors du chargement du mapping: {e}[/bold red]")
        
        # Cr√©er le processeur
        processor = GenericJsonProcessor(
            field_mappings=field_mappings,
            detect_fields=detect,
            extract_keywords=True,
            use_llm_fallback=use_llm,
            llm_model=llm_model,
            preserve_source=preserve_source
        )
        
        # Traiter le fichier
        result = processor.process_file(input_file, output_file)
        
        if result:
            console.print(f"[bold green]‚úÖ Fichier trait√© avec succ√®s: [cyan]{output_file}[/cyan][/bold green]")
            
            # G√©n√©rer un r√©sum√© LLM si l'option est activ√©e et que des donn√©es sont disponibles
            if use_llm:
                try:
                    # Charger les donn√©es enrichies
                    with open(output_file, 'r', encoding='utf-8') as f:
                        processed_data = json.load(f)
                    
                    # D√©terminer le r√©pertoire de sortie √† partir du fichier de sortie
                    output_dir = os.path.dirname(output_file)
                    if not output_dir:
                        output_dir = "."
                    
                    # G√©n√©rer le r√©sum√© LLM
                    llm_summary_file = generate_llm_summary(
                        output_dir, 
                        data=processed_data, 
                        filename=f"{os.path.splitext(os.path.basename(output_file))[0]}_llm_summary.md"
                    )
                    console.print(f"[bold green]‚úÖ R√©sum√© LLM g√©n√©r√©: [cyan]{llm_summary_file}[/cyan][/bold green]")
                except Exception as e:
                    console.print(f"[bold yellow]‚ö†Ô∏è Impossible de g√©n√©rer le r√©sum√© LLM: {e}[/bold yellow]")
            
            return True
        else:
            console.print("[bold red]‚ùå Erreur lors du traitement du fichier[/bold red]")
            return False
    except Exception as e:
        console.print(f"[bold red]‚ùå Erreur: {e}[/bold red]")
        return False


def show_summary(file_path: str):
    """Affiche un r√©sum√© du fichier trait√© avec des statistiques."""
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        # Cr√©er un tableau de statistiques
        table = Table(title="R√©sum√© du traitement")
        
        table.add_column("M√©trique", style="cyan")
        table.add_column("Valeur", style="green")
        
        # Nombre d'√©l√©ments
        items_count = len(data.get("items", []))
        table.add_row("√âl√©ments trait√©s", str(items_count))
        
        # Source
        if "metadata" in data and "source_file" in data["metadata"]:
            table.add_row("Fichier source", data["metadata"]["source_file"])
        
        # Date de traitement
        if "metadata" in data and "processed_at" in data["metadata"]:
            table.add_row("Date de traitement", data["metadata"]["processed_at"])
        
        # Statistiques
        if "metadata" in data and "stats" in data["metadata"]:
            for key, value in data["metadata"]["stats"].items():
                table.add_row(key.replace("_", " ").capitalize(), str(value))
        
        # Premier √©l√©ment en exemple (si pr√©sent)
        if items_count > 0:
            first_item = data["items"][0]
            
            # ID et titre
            if "id" in first_item:
                table.add_row("Premier √©l√©ment ID", str(first_item["id"]))
            if "title" in first_item:
                title = first_item["title"]
                if len(title) > 50:
                    title = title[:47] + "..."
                table.add_row("Premier √©l√©ment titre", title)
        
        console.print(table)
        
        # Afficher un aper√ßu du fichier JSON
        console.print("\n[bold]Aper√ßu du JSON g√©n√©r√©:[/bold]")
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
            # Limiter √† un extrait raisonnable
            content_preview = content[:5000] + ("\n..." if len(content) > 5000 else "")
            syntax = Syntax(content_preview, "json", theme="monokai", line_numbers=True)
            console.print(syntax)
        
        console.print(f"\nFichier de sortie : [bold cyan]{file_path}[/bold cyan]")
        console.print("\n[bold green]Traitement termin√© avec succ√®s ![/bold green]")
        
    except Exception as e:
        console.print(f"[bold red]Erreur lors de l'affichage du r√©sum√©: {e}[/bold red]")


@app.command()
def match(
    jira_file: str = typer.Argument(..., help="Fichier JSON de tickets JIRA trait√©s"),
    confluence_file: str = typer.Argument(..., help="Fichier JSON de pages Confluence trait√©es"),
    output_dir: str = typer.Option(None, "--output-dir", "-o", help="R√©pertoire de sortie"),
    min_score: float = typer.Option(None, "--min-score", "-s", help="Score minimum pour les correspondances"),
    llm_assist: bool = typer.Option(False, "--llm-assist", help="Utiliser un LLM pour am√©liorer les correspondances"),
    api_key: Optional[str] = typer.Option(None, "--api-key", help="Cl√© API pour le LLM (ou variable d'environnement OPENAI_API_KEY)"),
    llm_model: str = typer.Option(None, "--model", help="Mod√®le LLM √† utiliser"),
    compress: bool = typer.Option(False, "--compress", help="Compresser les fichiers de sortie avec zstd et orjson"),
    compress_level: int = typer.Option(19, "--compress-level", help="Niveau de compression zstd (1-22)"),
    keep_originals: bool = typer.Option(True, "--keep-originals/--no-originals", help="Conserver les fichiers JSON originaux en plus des compress√©s"),
):
    """
    √âtablit des correspondances entre tickets JIRA et pages Confluence.
    """
    # G√©n√©rer un timestamp pour le nom du dossier
    from datetime import datetime
    timestamp = datetime.now().strftime("%Y-%m-%d-%H-%M-%S")
    
    # Obtenir le chemin absolu vers le r√©pertoire racine du projet
    project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    
    # Utiliser le dossier results comme base √† la racine du projet
    base_results_dir = os.path.join(project_root, "results")
    os.makedirs(base_results_dir, exist_ok=True)
    
    # Si output_dir est sp√©cifi√©, l'utiliser comme nom de dossier avec le timestamp
    if output_dir:
        output_dir = os.path.join(base_results_dir, f"{output_dir}_{timestamp}")
    else:
        # Cr√©er un nom √† partir des noms de fichiers
        jira_name = os.path.splitext(os.path.basename(jira_file))[0]
        confluence_name = os.path.splitext(os.path.basename(confluence_file))[0]
        output_dir = os.path.join(base_results_dir, f"matches_{jira_name}_{confluence_name}_{timestamp}")
    
    # Valeur par d√©faut pour min_score
    min_score = min_score or float(os.environ.get("MIN_MATCH_SCORE", "0.2"))
    
    # V√©rifier que les fichiers existent
    if not os.path.exists(jira_file):
        console.print(f"[bold red]Le fichier JIRA {jira_file} n'existe pas.[/bold red]")
        raise typer.Exit(1)
    
    if not os.path.exists(confluence_file):
        console.print(f"[bold red]Le fichier Confluence {confluence_file} n'existe pas.[/bold red]")
        raise typer.Exit(1)
    
    # Cr√©er le r√©pertoire de sortie
    ensure_dir(output_dir)
    
    # D√©finir les chemins de sortie
    matches_file = os.path.join(output_dir, "jira_confluence_matches.json")
    jira_with_matches_file = os.path.join(output_dir, "jira_with_matches.json")
    confluence_with_matches_file = os.path.join(output_dir, "confluence_with_matches.json")
    
    # Afficher les infos
    console.print(Panel.fit(
        f"[bold]Matching JIRA ‚Üî Confluence[/bold]\n\n"
        f"JIRA : [cyan]{jira_file}[/cyan]\n"
        f"Confluence : [cyan]{confluence_file}[/cyan]\n"
        f"Score minimum : [cyan]{min_score}[/cyan]",
        title="Matching",
        border_style="blue"
    ))
    
    # Construire le chemin vers le script match_jira_confluence.py
    match_script = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), 
                             "extract", "match_jira_confluence.py")
    
    # Ex√©cuter le script de matching
    cmd = [
        sys.executable, match_script,
        "--jira", jira_file,
        "--confluence", confluence_file,
        "--output", matches_file,
        "--updated-jira", jira_with_matches_file,
        "--updated-confluence", confluence_with_matches_file,
        "--min-score", str(min_score)
    ]
    
    with console.status("[bold blue]Matching en cours..."):
        from subprocess import run, PIPE
        result = run(cmd, stdout=PIPE, stderr=PIPE, text=True)
    
    if result.returncode != 0:
        console.print("[bold red]Erreur lors du matching:[/bold red]")
        console.print(result.stderr)
        raise typer.Exit(1)
    
    # Afficher les r√©sultats
    console.print(result.stdout)
    
    # LLM pour am√©liorer les correspondances si demand√©
    if llm_assist:
        # Utiliser les valeurs par d√©faut si non sp√©cifi√©es
        if not api_key:
            api_key = os.environ.get("OPENAI_API_KEY")
            
        if not llm_model:
            llm_model = os.environ.get("DEFAULT_LLM_MODEL", "gpt-4.1")
            
        if not api_key:
            console.print("[bold yellow]Pas de cl√© API OpenAI trouv√©e. L'assistance LLM ne sera pas effectu√©e.[/bold yellow]")
            llm_assist = False
        
        if llm_assist:
            console.print("[bold]Am√©lioration des correspondances avec LLM...[/bold]")
            
            # TODO: Impl√©menter l'am√©lioration des correspondances avec LLM
            # Cela pourrait inclure:
            # 1. Analyse des correspondances de faible score pour confirmer/infirmer
            # 2. Recherche de correspondances suppl√©mentaires bas√©es sur la s√©mantique
            # 3. Suggestion de nouveaux liens qui n'ont pas √©t√© d√©tect√©s automatiquement
            
            console.print("[bold yellow]Assistance LLM pour les correspondances non impl√©ment√©e.[/bold yellow]")
    
    # Afficher un r√©sum√©
    try:
        with open(matches_file, 'r', encoding='utf-8') as f:
            matches = json.load(f)
        
        table = Table(title="R√©sum√© des correspondances")
        table.add_column("M√©trique", style="cyan")
        table.add_column("Valeur", style="green")
        
        # Nombre de tickets avec correspondances
        total_tickets = len(matches)
        table.add_row("Tickets avec correspondances", str(total_tickets))
        
        # Nombre total de correspondances
        total_matches = sum(len(matches[ticket_id]) for ticket_id in matches)
        table.add_row("Correspondances totales", str(total_matches))
        
        # Moyenne de correspondances par ticket
        avg_matches = total_matches / total_tickets if total_tickets > 0 else 0
        table.add_row("Moyenne par ticket", f"{avg_matches:.2f}")
        
        console.print(table)
        
        console.print(f"\nFichiers g√©n√©r√©s dans : [bold cyan]{output_dir}[/bold cyan]")
        console.print("\n[bold green]Matching termin√© avec succ√®s ![/bold green]")
        
    except Exception as e:
        console.print(f"[bold red]Erreur lors de l'affichage du r√©sum√©: {e}[/bold red]")


@app.command()
def unified(
    jira_files: List[str] = typer.Argument(..., help="Fichiers JSON JIRA √† traiter"),
    confluence_files: List[str] = typer.Option([], "--confluence", "-c", help="Fichiers JSON Confluence √† traiter"),
    output_dir: str = typer.Option(None, "--output-dir", "-o", help="R√©pertoire de sortie"),
    min_match_score: float = typer.Option(None, "--min-score", "-s", help="Score minimum pour les correspondances"),
    max_items: Optional[int] = typer.Option(None, "--max", help="Nombre maximum d'√©l√©ments √† traiter par fichier"),
    use_llm: bool = typer.Option(True, "--llm/--no-llm", help="Utiliser un LLM pour l'enrichissement"),
    api_key: Optional[str] = typer.Option(None, "--api-key", help="Cl√© API OpenAI (si non d√©finie dans les variables d'environnement)"),
    skip_matching: bool = typer.Option(False, "--skip-matching", help="Ne pas effectuer le matching entre JIRA et Confluence"),
    language: Optional[str] = typer.Option(None, "--lang", help="Langue √† utiliser (fr/en)"),
    compress: bool = typer.Option(True, "--compress/--no-compress", help="Compresser les fichiers de sortie avec zstd et orjson (activ√© par d√©faut)"),
    compress_level: int = typer.Option(19, "--compress-level", help="Niveau de compression zstd (1-22)"),
    keep_originals: bool = typer.Option(True, "--keep-originals/--no-originals", help="Conserver les fichiers JSON originaux en plus des compress√©s"),
):
    """
    Flux unifi√©: traite JIRA + Confluence, fait le matching et pr√©pare les donn√©es pour le LLM.
    
    Cette commande ex√©cute un workflow complet pour traiter les fichiers JIRA et Confluence:
    1. Traitement des fichiers JIRA pour extraction des donn√©es cl√©s
    2. Traitement des fichiers Confluence si fournis
    3. Matching entre Confluence et JIRA (sauf si --skip-matching est activ√©)
    4. Pr√©paration des donn√©es pour enrichissement LLM
    5. Compression des fichiers (activ√©e par d√©faut)
    """
    # G√©n√©rer un timestamp pour le nom du dossier
    from datetime import datetime
    timestamp = datetime.now().strftime("%Y%m%d%H%M%S")
    
    # Obtenir le chemin absolu vers le r√©pertoire racine du projet
    project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    
    # Utiliser le dossier results comme base √† la racine du projet
    base_results_dir = os.path.join(project_root, "results")
    os.makedirs(base_results_dir, exist_ok=True)
    
    # G√©rer le r√©pertoire de sortie
    if output_dir:
        # Si on a un dossier de sortie pr√©cis, l'utiliser avec le timestamp
        full_output_dir = os.path.join(base_results_dir, f"{output_dir}_{timestamp}")
    else:
        # Cr√©er un r√©pertoire par d√©faut avec timestamp
        full_output_dir = os.path.join(base_results_dir, f"{timestamp}-unified-process")
    
    # Cr√©er le r√©pertoire
    ensure_dir(full_output_dir)
    print_info(f"R√©pertoire de sortie: {full_output_dir}")
    
    # D√©finir la valeur par d√©faut pour min_match_score si non sp√©cifi√©e
    min_score = min_match_score or float(os.environ.get("MIN_MATCH_SCORE", "0.2"))
    print_info(f"Score minimum pour les correspondances: {min_score}")
    
    # S'assurer que la langue est configur√©e
    if language:
        language_str = str(language)
        # Ne pas afficher l'objet OptionInfo, juste la valeur
        console.print(f"üåê Langue configur√©e: {language_str}")
    
    # Options pour run_unified_analysis.py
    cmd = [
        sys.executable,
        os.path.join(parent_dir, "extract", "run_unified_analysis.py"),
        "--jira-files"
    ] + jira_files
    
    if confluence_files:
        cmd.extend(["--confluence-files"] + confluence_files)
    
    cmd.extend([
        "--output-dir", str(full_output_dir),
        "--min-match-score", str(min_score)
    ])
    
    if max_items:
        cmd.extend(["--max-items", str(max_items)])
    
    if use_llm:
        cmd.append("--with-openai")
        if api_key:
            cmd.extend(["--api-key", str(api_key)])
    else:
        cmd.append("--no-openai")
    
    if skip_matching:
        cmd.append("--skip-matching")
    
    if language:
        cmd.extend(["--language", str(language)])
    
    # Toujours ajouter les options de compression
    # M√™me si compress est False dans les param√®tres, on force l'activation
        cmd.append("--compress")
        cmd.extend(["--compress-level", str(compress_level)])
    # Force keep_originals √† True
    cmd.append("--keep-originals")
    
    # Ex√©cuter le script run_unified_analysis.py
    try:
        console.print("\n[bold cyan]Ex√©cution du flux unifi√©...[/bold cyan]")
        process = subprocess.run(cmd, text=True, capture_output=True)
        
        if process.returncode == 0:
            console.print(process.stdout)
            
            # Corriger les √©ventuels probl√®mes de dossiers dupliqu√©s
            try:
                # Importer le module tools
                from tools import fix_duplicate_paths
                
                # Appliquer la correction au r√©pertoire de sortie
                moved_count = fix_duplicate_paths(full_output_dir)
                if moved_count > 0:
                    console.print(f"[yellow]‚ö†Ô∏è Correction de {moved_count} fichiers dans des chemins dupliqu√©s[/yellow]")
            except ImportError:
                console.print("[yellow]Module tools non trouv√©. Pas de correction de chemins dupliqu√©s.[/yellow]")
            except Exception as e:
                console.print(f"[yellow]Erreur lors de la correction des chemins: {str(e)}[/yellow]")
            
            print_success(f"Fichiers g√©n√©r√©s dans : {full_output_dir}")
            console.print(f"Arborescence globale g√©n√©r√©e dans : {os.path.join(full_output_dir, 'global_arborescence.txt')}")
            
            # Afficher des informations sur la compression
            current_lang = get_current_language()
            report_path = os.path.join(full_output_dir, f"compression_report_{current_lang}.txt")
            if os.path.exists(report_path):
                console.print(f"[bold cyan]Rapport de compression g√©n√©r√© dans : [/bold cyan]{report_path}")
            
            print_success("Traitement unifi√© termin√© avec succ√®s !")
        else:
            console.print(process.stdout)
            console.print(process.stderr)
            print_error("Le traitement a √©chou√©. Consultez les messages d'erreur ci-dessus.")
            
    except Exception as e:
        print_error(f"Erreur lors de l'ex√©cution du flux unifi√©: {str(e)}")
        return


@app.command()
def chunks(
    input_file: str = typer.Argument(..., help="Fichier JSON volumineux √† d√©couper"),
    output_dir: str = typer.Option(None, "--output-dir", "-o", help="R√©pertoire de sortie pour les morceaux"),
    items_per_file: int = typer.Option(500, "--items-per-file", "-n", help="Nombre d'√©l√©ments par fichier"),
    process_after: bool = typer.Option(False, "--process", "-p", help="Traiter chaque morceau apr√®s d√©coupage"),
    mapping_file: Optional[str] = typer.Option(None, "--mapping", "-m", help="Fichier de mapping √† utiliser pour le traitement"),
    use_llm: bool = typer.Option(False, "--llm", help="Utiliser un LLM pour l'enrichissement lors du traitement"),
    compress: bool = typer.Option(False, "--compress", help="Compresser les fichiers de sortie avec zstd et orjson"),
    compress_level: int = typer.Option(19, "--compress-level", help="Niveau de compression zstd (1-22)"),
    keep_originals: bool = typer.Option(True, "--keep-originals/--no-originals", help="Conserver les fichiers JSON originaux en plus des compress√©s"),
):
    """
    D√©coupe un fichier JSON volumineux en morceaux plus petits et optionnellement les traite.
    
    Utilise process_by_chunks.py pour g√©rer efficacement les gros fichiers JSON.
    """
    # Valider les arguments
    if not os.path.exists(input_file):
        console.print(f"[bold red]Le fichier {input_file} n'existe pas.[/bold red]")
        raise typer.Exit(1)
    
    # G√©n√©rer un timestamp pour le nom du dossier
    from datetime import datetime
    timestamp = datetime.now().strftime("%Y-%m-%d-%H-%M-%S")
    
    # Obtenir le chemin absolu vers le r√©pertoire racine du projet
    project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    
    # Utiliser le dossier results comme base √† la racine du projet
    base_results_dir = os.path.join(project_root, "results")
    os.makedirs(base_results_dir, exist_ok=True)
    
    # Si output_dir est sp√©cifi√©, l'utiliser comme nom de dossier avec le timestamp
    if output_dir:
        output_dir = os.path.join(base_results_dir, f"{output_dir}_{timestamp}")
    else:
        # Utiliser le nom du fichier d'entr√©e comme nom de base pour le dossier de sortie
        file_base_name = os.path.splitext(os.path.basename(input_file))[0]
        output_dir = os.path.join(base_results_dir, f"chunks_{file_base_name}_{timestamp}")
    
    ensure_dir(output_dir)
    
    # Construire le chemin vers le script process_by_chunks.py
    chunks_script = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), 
                              "extract", "process_by_chunks.py")
    
    console.print(Panel.fit(
        "[bold]D√©coupage de fichier JSON volumineux[/bold]\n\n"
        f"Fichier d'entr√©e : [cyan]{input_file}[/cyan]\n"
        f"R√©pertoire de sortie : [cyan]{output_dir}[/cyan]\n"
        f"√âl√©ments par fichier : [cyan]{items_per_file}[/cyan]",
        title="D√©coupage en morceaux",
        border_style="blue"
    ))
    
    # Commande pour d√©couper le fichier
    cmd = [
        sys.executable, chunks_script,
        "split",
        "--input", input_file,
        "--output-dir", output_dir,
        "--items-per-file", str(items_per_file)
    ]
    
    # Ex√©cuter la commande
    with console.status("[bold blue]D√©coupage en cours..."):
        from subprocess import run, PIPE
        result = run(cmd, stdout=PIPE, stderr=PIPE, text=True)
    
    if result.returncode != 0:
        console.print("[bold red]Erreur lors du d√©coupage:[/bold red]")
        console.print(result.stderr)
        raise typer.Exit(1)
    
    console.print("[bold green]D√©coupage termin√© avec succ√®s ![/bold green]")
    
    # Afficher les fichiers g√©n√©r√©s
    chunk_files = glob.glob(os.path.join(output_dir, "*.json"))
    console.print(f"[bold]Nombre de morceaux cr√©√©s:[/bold] {len(chunk_files)}")
    
    # Traiter chaque morceau si demand√©
    if process_after and chunk_files:
        console.print("\n[bold]Traitement des morceaux...[/bold]")
        
        # Cr√©er le r√©pertoire pour les fichiers trait√©s
        processed_dir = os.path.join(output_dir, "processed")
        ensure_dir(processed_dir)
        
        # Traiter chaque fichier
        with Progress(
            SpinnerColumn(),
            TextColumn("[bold blue]{task.description}"),
            BarColumn(),
            TextColumn("[bold]{task.completed}/{task.total}"),
            TimeElapsedColumn(),
        ) as progress:
            task = progress.add_task("[cyan]Traitement des morceaux...", total=len(chunk_files))
            
            for chunk_file in chunk_files:
                chunk_name = os.path.basename(chunk_file)
                output_file = os.path.join(processed_dir, f"{os.path.splitext(chunk_name)[0]}_processed.json")
                
                # Construire la commande process
                process_args = {
                    "input_file": chunk_file,
                    "output_file": output_file,
                    "mapping_file": mapping_file,
                    "use_llm": use_llm
                }
                
                # Ex√©cuter process() sur le morceau
                try:
                    process(**process_args)
                    progress.update(task, advance=1)
                except Exception as e:
                    console.print(f"[yellow]Erreur lors du traitement de {chunk_name}: {e}[/yellow]")
                    progress.update(task, advance=1)
        
        console.print(f"\n[bold green]Traitement des morceaux termin√© ![/bold green]")
        console.print(f"Fichiers trait√©s disponibles dans : [bold cyan]{processed_dir}[/bold cyan]")


@app.command()
def interactive(
    language: str = typer.Option(None, "--lang", "-l", help="Langue de l'interface (fr/en)")
):
    """
    Mode enti√®rement interactif qui guide l'utilisateur √† travers toutes les √©tapes.
    
    Ce mode permet de choisir le type d'op√©ration √† effectuer et guide
    l'utilisateur √† travers toutes les options de mani√®re conviviale.
    """
    if language:
        set_language(language)
        
    console.print(Panel.fit(
        t("interactive_content", "panels"),
        title=t("interactive_title", "panels"),
        border_style="green"
    ))
    
    # 1. S√©lection du type d'op√©ration
    operation_choices = [
        t("process", "interactive_choices"),
        t("chunks", "interactive_choices"),
        t("match", "interactive_choices"),
        t("unified", "interactive_choices"),
        t("clean", "interactive_choices"),
        t("compress", "interactive_choices"),
        t("describe_images", "interactive_choices"),
        # Ajouter l'option pour changer de langue
        f"üåê {t('change_language', 'interactive_choices', 'fr')} / {t('change_language', 'interactive_choices', 'en')}",
        t("quit", "interactive_choices")
    ]
    
    questions = [
        inquirer.List('operation',
                     message=t("select_operation", "messages"),
                     choices=operation_choices)
    ]
    answers = inquirer.prompt(questions)
    
    if not answers:
        return
        
    selected_operation = answers['operation']
    
    # V√©rifier si l'utilisateur veut changer de langue
    if "üåê" in selected_operation:
        current_lang = get_current_language()
        new_lang = "en" if current_lang == "fr" else "fr"
        
        console.print(f"[bold]Changing language to {new_lang}[/bold]" if new_lang == "en" else f"[bold]Changement de langue vers {new_lang}[/bold]")
        set_language(new_lang)
        
        # Relancer le menu avec la nouvelle langue explicitement sp√©cifi√©e
        return interactive(language=new_lang)
    
    # Quitter si demand√©
    if selected_operation == t("quit", "interactive_choices"):
        return
    
    # 2. Selon l'op√©ration choisie, poser les questions appropri√©es
    if t("process", "interactive_choices") in selected_operation:
        _run_interactive_process()
    elif t("chunks", "interactive_choices") in selected_operation:
        _run_interactive_chunks()
    elif t("match", "interactive_choices") in selected_operation:
        _run_interactive_match()
    elif t("unified", "interactive_choices") in selected_operation:
        _run_interactive_unified()
    elif t("clean", "interactive_choices") in selected_operation:
        _run_interactive_clean()
    elif t("compress", "interactive_choices") in selected_operation:
        _run_interactive_compress()
    elif t("describe_images", "interactive_choices") in selected_operation:
        _run_interactive_describe_images()


def _run_interactive_process():
    """Interface interactive pour la commande process."""
    # S√©lection du fichier d'entr√©e
    input_file = _prompt_for_file("S√©lectionnez le fichier JSON √† traiter:")
    if not input_file:
        return
    
    # D√©tection automatique du type
    file_type = detect_file_type(input_file)
    if file_type:
        console.print(f"Type d√©tect√© : [bold green]{file_type['type']}[/bold green]")
    
    # Demander le fichier de sortie
    default_output = f"{os.path.splitext(input_file)[0]}_processed.json"
    output_file = typer.prompt("Fichier de sortie", default=default_output)
    
    # Chercher les mappings disponibles
    mapping_choices = find_mapping_files()
    mapping_choices.insert(0, "Sans mapping (d√©tection automatique)")
    mapping_choices.append("Cr√©er un mapping personnalis√©")
    
    # S√©lection du mapping
    questions = [
        inquirer.List('mapping_choice',
                     message="Choisissez un mapping pour le traitement",
                     choices=mapping_choices,
                     default="Sans mapping (d√©tection automatique)" if not file_type else f"{file_type['type']}_mapping.json" if f"{file_type['type']}_mapping.json" in mapping_choices else "Sans mapping (d√©tection automatique)")
    ]
    answers = inquirer.prompt(questions)
    
    mapping_file = None
    if answers["mapping_choice"] == "Sans mapping (d√©tection automatique)":
        mapping_file = None
    elif answers["mapping_choice"] == "Cr√©er un mapping personnalis√©":
        mapping_file = _create_custom_mapping()
    else:
        mapping_file = os.path.join(DEFAULT_MAPPINGS_DIR, answers["mapping_choice"])
    
    # Options avanc√©es
    questions = [
        inquirer.Confirm('use_llm',
                        message="Voulez-vous enrichir avec LLM (OpenAI) ?",
                        default=False),
        inquirer.Text('max_items',
                     message="Nombre maximum d'√©l√©ments √† traiter (vide = tous)",
                     default=""),
        inquirer.Confirm('preserve_source',
                         message="Pr√©server les fichiers sources originaux ?",
                         default=True)
    ]
    answers_advanced = inquirer.prompt(questions)
    
    use_llm = answers_advanced['use_llm']
    max_items = int(answers_advanced['max_items']) if answers_advanced['max_items'].strip() else None
    preserve_source = answers_advanced['preserve_source']
    
    # LLM si demand√©
    llm_model = None
    api_key = None
    if use_llm:
        # Cl√© API
        api_key = os.environ.get("OPENAI_API_KEY")
        if not api_key:
            api_key = typer.prompt("Entrez votre cl√© API OpenAI", hide_input=True)
        
        # Mod√®le
        questions = [
            inquirer.List('llm_model',
                         message="Choisissez un mod√®le LLM",
                         choices=LLM_MODELS,
                         default="gpt-4.1")
        ]
        llm_answers = inquirer.prompt(questions)
        llm_model = llm_answers['llm_model']
    
    # Confirmation finale
    console.print("\n[bold]R√©capitulatif :[/bold]")
    console.print(f"- Fichier d'entr√©e : [cyan]{input_file}[/cyan]")
    console.print(f"- Fichier de sortie : [cyan]{output_file}[/cyan]")
    console.print(f"- Mapping : [cyan]{mapping_file or 'Auto-d√©tection'}[/cyan]")
    console.print(f"- Pr√©servation des sources : [cyan]{'Oui' if preserve_source else 'Non'}[/cyan]")
    if use_llm:
        console.print(f"- Enrichissement LLM : [cyan]Oui ({llm_model})[/cyan]")
    if max_items:
        console.print(f"- Limite d'√©l√©ments : [cyan]{max_items}[/cyan]")
    
    questions = [
        inquirer.Confirm('confirm',
                        message="Lancer le traitement ?",
                        default=True)
    ]
    confirm = inquirer.prompt(questions)
    
    if not confirm or not confirm['confirm']:
        console.print("[yellow]Op√©ration annul√©e.[/yellow]")
        return
    
    # Ex√©cuter process avec les param√®tres d√©finis
    process(
        input_file=input_file,
        output_file=output_file,
        mapping_file=mapping_file,
        detect=True,
        auto_mapping=True,
        use_llm=use_llm,
        llm_model=llm_model,
        api_key=api_key,
        interactive=False,  # D√©j√† en mode interactif
        max_items=max_items,
        root_key="items",
        preserve_source=preserve_source,
        outlines=False
    )


def _run_interactive_chunks():
    """Interface interactive pour la commande chunks."""
    # S√©lection du fichier d'entr√©e
    input_file = _prompt_for_file("S√©lectionnez le fichier JSON volumineux √† d√©couper:")
    if not input_file:
        return
    
    # R√©pertoire de sortie
    default_output_dir = os.path.join(DEFAULT_OUTPUT_DIR, "chunks")
    output_dir = typer.prompt("R√©pertoire de sortie pour les morceaux", default=default_output_dir)
    
    # Options de d√©coupage
    questions = [
        inquirer.Text('items_per_file',
                     message="Nombre d'√©l√©ments par fichier",
                     default="500"),
        inquirer.Confirm('process_after',
                        message="Traiter les morceaux apr√®s d√©coupage ?",
                        default=False),
        inquirer.Confirm('compress',
                        message="Activer la compression des fichiers JSON ?",
                        default=False)
    ]
    answers = inquirer.prompt(questions)
    
    items_per_file = int(answers['items_per_file'])
    process_after = answers['process_after']
    compress = answers['compress']
    
    # Options de compression si activ√©e
    compress_level = 19  # Valeur par d√©faut
    keep_originals = True  # Valeur par d√©faut
    if compress:
        compression_questions = [
            inquirer.List('compress_level',
                          message=t("compression_level_prompt", "compression"),
                          choices=["15 (fast)", "19 (balanced)", "22 (max)"],
                          default="19 (balanced)"),
            inquirer.Confirm('keep_originals',
                             message=t("keep_originals_prompt", "compression"),
                             default=True)
        ]
        compression_answers = inquirer.prompt(compression_questions)
        compress_level = int(compression_answers['compress_level'].split(" ")[0])
        keep_originals = compression_answers['keep_originals']
    
    # Options de traitement si demand√©
    mapping_file = None
    use_llm = False
    if process_after:
        # Mapping
        mapping_choices = find_mapping_files()
        mapping_choices.insert(0, "Sans mapping (d√©tection automatique)")
        
        questions = [
            inquirer.List('mapping_choice',
                         message="Mapping pour le traitement des morceaux",
                         choices=mapping_choices),
            inquirer.Confirm('use_llm',
                            message="Enrichir avec LLM ?",
                            default=False)
        ]
        process_answers = inquirer.prompt(questions)
        
        if process_answers['mapping_choice'] != "Sans mapping (d√©tection automatique)":
            mapping_file = os.path.join(DEFAULT_MAPPINGS_DIR, process_answers['mapping_choice'])
        
        use_llm = process_answers['use_llm']
    
    # Confirmation finale
    console.print("\n[bold]R√©capitulatif :[/bold]")
    console.print(f"- Fichier d'entr√©e : [cyan]{input_file}[/cyan]")
    console.print(f"- R√©pertoire de sortie : [cyan]{output_dir}[/cyan]")
    console.print(f"- √âl√©ments par fichier : [cyan]{items_per_file}[/cyan]")
    if process_after:
        console.print(f"- Traitement apr√®s d√©coupage : [cyan]Oui[/cyan]")
        console.print(f"- Mapping : [cyan]{mapping_file or 'Auto-d√©tection'}[/cyan]")
        console.print(f"- Enrichissement LLM : [cyan]{'Oui' if use_llm else 'Non'}[/cyan]")
    if compress:
        console.print(f"- Compression JSON : [cyan]Oui (niveau {compress_level}, conserver originaux: {t('yes', 'messages') if keep_originals else t('no', 'messages')})[/cyan]")
    else:
        console.print(f"- Compression JSON : [cyan]Non[/cyan]")
    
    questions = [
        inquirer.Confirm('confirm',
                        message="Lancer le d√©coupage ?",
                        default=True)
    ]
    confirm = inquirer.prompt(questions)
    
    if not confirm or not confirm['confirm']:
        console.print("[yellow]Op√©ration annul√©e.[/yellow]")
        return
    
    # Ex√©cuter chunks avec les param√®tres d√©finis
    chunks(
        input_file=input_file,
        output_dir=output_dir,
        items_per_file=items_per_file,
        process_after=process_after,
        mapping_file=mapping_file,
        use_llm=use_llm,
        compress=compress,
        compress_level=compress_level if compress else None,
        keep_originals=keep_originals if compress else None
    )


def _create_custom_mapping() -> str:
    """Cr√©e un fichier de mapping personnalis√©."""
    temp_mapping_file = "temp_mapping.json"
    template = '''{
  "id": "key_field",
  "title": "title_field",
  "content": {
    "field": "content_field",
    "transform": "clean_text"
  },
  "metadata": {
    "created_by": "author_field",
    "created_at": "date_field"
  }
}'''
    with open(temp_mapping_file, 'w') as f:
        f.write(template)
    console.print("[bold]Cr√©ez votre mapping personnalis√© dans l'√©diteur.[/bold]")
    console.print("[yellow]Appuyez sur Entr√©e pour continuer apr√®s avoir sauvegard√© et ferm√© l'√©diteur.[/yellow]")
    # Ouvrir l'√©diteur par d√©faut
    if sys.platform == 'win32':
        os.system(f'notepad {temp_mapping_file}')
    else:
        editor = os.environ.get('EDITOR', 'nano')
        os.system(f'{editor} {temp_mapping_file}')
    input("Appuyez sur Entr√©e pour continuer...")
    # V√©rifier que le mapping est valide
    try:
        with open(temp_mapping_file, 'r') as f:
            json.load(f)
        return temp_mapping_file
    except json.JSONDecodeError:
        console.print("[bold red]Le mapping cr√©√© n'est pas un JSON valide.[/bold red]")
        return None


@app.command()
def clean(
    input_file: str = typer.Argument(..., help="Fichier ou dossier √† nettoyer"),
    output_file: Optional[str] = typer.Option(None, "--output", "-o", help="Fichier ou dossier de sortie"),
    recursive: bool = typer.Option(True, "--recursive/--no-recursive", help="Traiter r√©cursivement les objets JSON"),
    report: bool = typer.Option(False, "--report/--no-report", help="G√©n√©rer un rapport des donn√©es sensibles d√©tect√©es"),
):
    """
    Nettoie les donn√©es sensibles (cl√©s API, identifiants, etc.) des fichiers JSON.
    Utile pour pr√©parer des donn√©es de test ou avant de commit des fichiers.
    """
    console = Console()
    
    # V√©rifier que le chemin existe
    if not os.path.exists(input_file):
        console.print(f"[bold red]Le chemin {input_file} n'existe pas.[/bold red]")
        raise typer.Exit(1)
    
    # Importer le module clean_sensitive_data
    try:
        from tools.clean_sensitive_data import clean_json_file
    except ImportError:
        console.print("[bold red]Module tools non trouv√©. Installez-le avec pip install -e .[/bold red]")
        raise typer.Exit(1)
    
    # Afficher les infos
    console.print(Panel.fit(
        "[bold]Nettoyage des donn√©es sensibles[/bold]\n\n"
        f"Chemin d'entr√©e : [cyan]{input_file}[/cyan]\n"
        f"Chemin de sortie : [cyan]{output_file or 'Auto-g√©n√©r√©'}[/cyan]\n"
        f"R√©cursif : [cyan]{'Oui' if recursive else 'Non'}[/cyan]\n"
        f"Rapport : [cyan]{'Oui' if report else 'Non'}[/cyan]",
        title="Nettoyage de donn√©es",
        border_style="blue"
    ))
    
    # Traitement du fichier ou dossier
    input_path = Path(input_file)
    
    if input_path.is_file():
        output_path = Path(output_file) if output_file else None
        
        with console.status("[bold blue]Nettoyage en cours...[/bold blue]"):
            success = clean_json_file(input_path, output_path, recursive, report)
        
        if success:
            console.print(f"[bold green]‚úÖ Nettoyage termin√© avec succ√®s![/bold green]")
        else:
            console.print(f"[bold red]‚ùå Erreur lors du nettoyage![/bold red]")
            raise typer.Exit(1)
    
    elif input_path.is_dir():
        output_dir = Path(output_file) if output_file else input_path / "cleaned"
        os.makedirs(output_dir, exist_ok=True)
        
        json_files = list(input_path.glob("**/*.json"))
        
        if not json_files:
            console.print(f"[bold yellow]‚ö†Ô∏è Aucun fichier JSON trouv√© dans {input_path}[/bold yellow]")
            raise typer.Exit(0)
        
        console.print(f"[bold blue]üîç {len(json_files)} fichiers JSON trouv√©s[/bold blue]")
        
        with Progress() as progress:
            task = progress.add_task("[cyan]Nettoyage des fichiers...", total=len(json_files))
            
            success_count = 0
            for file in json_files:
                rel_path = file.relative_to(input_path)
                output_file_path = output_dir / rel_path.parent / f"{file.stem}_clean{file.suffix}"
                os.makedirs(output_file_path.parent, exist_ok=True)
                
                if clean_json_file(file, output_file_path, recursive, report):
                    success_count += 1
                
                progress.update(task, advance=1)
        
        console.print(f"[bold green]‚úÖ {success_count}/{len(json_files)} fichiers nettoy√©s avec succ√®s![/bold green]")
        
        if success_count < len(json_files):
            console.print(f"[bold yellow]‚ö†Ô∏è {len(json_files) - success_count} fichiers n'ont pas pu √™tre nettoy√©s[/bold yellow]")
    
    else:
        console.print(f"[bold red]Le chemin {input_file} n'est ni un fichier ni un dossier.[/bold red]")
        raise typer.Exit(1)


def _run_interactive_clean():
    """Interface interactive pour la commande clean."""
    # S√©lection du fichier ou dossier d'entr√©e
    input_path = _prompt_for_file(t("select_file_clean", "prompts"))
    if not input_path:
        return
    
    # Options
    questions = [
        inquirer.Confirm('recursive',
                         message=t("recursive", "confirmations"),
                         default=True),
        inquirer.Text('output',
                      message=t("output_clean", "prompts"),
                      default="")
    ]
    answers = inquirer.prompt(questions)
    
    recursive = answers['recursive'] if os.path.isdir(input_path) else False
    output = answers['output'] if answers['output'].strip() else None
    
    # Confirmation
    console.print(f"\n[bold]{t('summary', 'messages')}[/bold]")
    console.print(f"- {t('input_path', 'messages')} [cyan]{input_path}[/cyan]")
    console.print(f"- {t('output_path', 'messages')} [cyan]{output or t('auto_detection', 'messages')}[/cyan]")
    if os.path.isdir(input_path):
        console.print(f"- {t('recursive', 'messages')} [cyan]{t('yes', 'messages') if recursive else t('no', 'messages')}[/cyan]")
    
    questions = [
        inquirer.Confirm('confirm',
                         message=t("launch_cleaning", "confirmations"),
                         default=True)
    ]
    confirm = inquirer.prompt(questions)
    
    if confirm and confirm['confirm']:
        clean(input_path, output, recursive)


@app.command()
def compress(
    directory: str = typer.Argument(..., help="R√©pertoire ou fichier JSON √† compresser/optimiser"),
    compress_level: int = typer.Option(19, "--level", "-l", help="Niveau de compression zstd (1-22)"),
    keep_originals: bool = typer.Option(True, "--keep-originals/--no-originals", help="Conserver les fichiers JSON originaux"),
    language: str = typer.Option(None, "--lang", help="Langue pour le rapport (fr/en)"),
):
    """
    Compresse et optimise des fichiers JSON avec zstd et orjson.
    R√©duit consid√©rablement la taille des fichiers et am√©liore les performances de lecture/√©criture.
    """
    console = Console()
    
    # V√©rifier que le chemin existe
    if not os.path.exists(directory):
        console.print(f"[bold red]Le chemin {directory} n'existe pas.[/bold red]")
        raise typer.Exit(1)
    
    # D√©finir la langue si sp√©cifi√©e
    if language and TRANSLATIONS_LOADED:
        set_language(language)
    
    # Importer le module de compression
    try:
        from extract.compress_utils import compress_results_directory
    except ImportError:
        console.print("[bold red]Module de compression non disponible. Installation des d√©pendances...[/bold red]")
        subprocess.run([sys.executable, "-m", "pip", "install", "zstandard", "orjson"], check=True)
        try:
            from extract.compress_utils import compress_results_directory
        except ImportError:
            console.print("[bold red]Impossible d'importer le module de compression.[/bold red]")
            raise typer.Exit(1)
    
    # Afficher les infos
    console.print(Panel.fit(
        f"[bold]{t('compress_minify_desc', 'compression')}[/bold]\n\n"
        f"{t('directory_help', 'compression')} : [cyan]{directory}[/cyan]\n"
        f"{t('compression_level_help', 'compression')} : [cyan]{compress_level}[/cyan]\n"
        f"{t('keep_originals_help', 'compression')} : [cyan]{t('yes', 'messages') if keep_originals else t('no', 'messages')}[/cyan]",
        title=t("compression", "interactive_choices"),
        border_style="blue"
    ))
    
    # Compresser les fichiers
    with console.status(f"[bold blue]{t('compressing_files', 'compression')} {directory}..."):
        try:
            count, report = compress_results_directory(
                directory, 
                compression_level=compress_level,
                keep_originals=keep_originals,
                generate_report=True
            )
        except Exception as e:
            console.print(f"[bold red]Erreur lors de la compression: {e}[/bold red]")
            import traceback
            console.print(traceback.format_exc())
            raise typer.Exit(1)
    
    # Afficher les r√©sultats
    current_lang = get_current_language()
    report_path = os.path.join(directory, f"compression_report_{current_lang}.txt")
    
    console.print(f"[bold green]‚úÖ {count} {t('files_compressed_success', 'compression')}![/bold green]")
    console.print(f"[bold]{t('report_available', 'compression')}:[/bold] {report_path}")
    
    # Si le rapport existe, afficher un r√©sum√©
    if os.path.exists(report_path):
        with open(report_path, 'r', encoding='utf-8') as f:
            report_content = f.read()
            
            # Extraire juste le r√©sum√© global
            summary_section = ""
            in_summary = False
            for line in report_content.split('\n'):
                if line.startswith("## Global Summary") or line.startswith("## R√©sum√© Global"):
                    in_summary = True
                    summary_section += line + "\n"
                elif in_summary and line.startswith("##"):
                    in_summary = False
                elif in_summary:
                    summary_section += line + "\n"
            
            # Afficher le r√©sum√©
            console.print("\n[bold cyan]R√©sum√© de compression:[/bold cyan]")
            console.print(Markdown(summary_section))


def _run_interactive_compress():
    """Interface interactive pour la commande compress."""
    # S√©lection du r√©pertoire ou fichier √† compresser
    directory = _prompt_for_file(t("select_dir_compress", "prompts"))
    if not directory:
        return
    
    # Options de compression
    questions = [
        inquirer.List('compress_level',
                      message=t("compression_level_prompt", "compression"),
                      choices=["15 (fast)", "19 (balanced)", "22 (max)"],
                      default="19 (balanced)"),
        inquirer.Confirm('keep_originals',
                         message=t("keep_originals_prompt", "compression"),
                         default=True)
    ]
    answers = inquirer.prompt(questions)
    
    # Extraire le niveau de compression
    compress_level = int(answers['compress_level'].split(" ")[0])
    keep_originals = answers['keep_originals']
    
    # Confirmation
    console.print(f"\n[bold]{t('summary', 'messages')}[/bold]")
    console.print(f"- {t('directory_help', 'compression')} : [cyan]{directory}[/cyan]")
    console.print(f"- {t('compression_level_help', 'compression')} : [cyan]{compress_level}[/cyan]")
    console.print(f"- {t('keep_originals_help', 'compression')} : [cyan]{t('yes', 'messages') if keep_originals else t('no', 'messages')}[/cyan]")
    
    questions = [
        inquirer.Confirm('confirm',
                         message=t("launch_compression", "confirmations"),
                         default=True)
    ]
    confirm = inquirer.prompt(questions)
    
    if confirm and confirm['confirm']:
        # Ex√©cuter la compression
        compress(directory, compress_level, keep_originals)


@app.command()
def match(
    jira_file: str = typer.Argument(..., help="Fichier JSON de tickets JIRA trait√©s"),
    confluence_file: str = typer.Argument(..., help="Fichier JSON de pages Confluence trait√©es"),
    output_dir: str = typer.Option(None, "--output-dir", "-o", help="R√©pertoire de sortie"),
    min_score: float = typer.Option(None, "--min-score", "-s", help="Score minimum pour les correspondances"),
    llm_assist: bool = typer.Option(False, "--llm-assist", help="Utiliser un LLM pour am√©liorer les correspondances"),
    api_key: Optional[str] = typer.Option(None, "--api-key", help="Cl√© API pour le LLM (ou variable d'environnement OPENAI_API_KEY)"),
    llm_model: str = typer.Option(None, "--model", help="Mod√®le LLM √† utiliser"),
    compress: bool = typer.Option(False, "--compress", help="Compresser les fichiers de sortie avec zstd et orjson"),
    compress_level: int = typer.Option(19, "--compress-level", help="Niveau de compression zstd (1-22)"),
    keep_originals: bool = typer.Option(True, "--keep-originals/--no-originals", help="Conserver les fichiers JSON originaux en plus des compress√©s"),
):
    """
    √âtablit des correspondances entre tickets JIRA et pages Confluence.
    """
    # G√©n√©rer un timestamp pour le nom du dossier
    from datetime import datetime
    timestamp = datetime.now().strftime("%Y-%m-%d-%H-%M-%S")
    
    # Obtenir le chemin absolu vers le r√©pertoire racine du projet
    project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    
    # Utiliser le dossier results comme base √† la racine du projet
    base_results_dir = os.path.join(project_root, "results")
    os.makedirs(base_results_dir, exist_ok=True)
    
    # Si output_dir est sp√©cifi√©, l'utiliser comme nom de dossier avec le timestamp
    if output_dir:
        output_dir = os.path.join(base_results_dir, f"{output_dir}_{timestamp}")
    else:
        # Cr√©er un nom √† partir des noms de fichiers
        jira_name = os.path.splitext(os.path.basename(jira_file))[0]
        confluence_name = os.path.splitext(os.path.basename(confluence_file))[0]
        output_dir = os.path.join(base_results_dir, f"matches_{jira_name}_{confluence_name}_{timestamp}")
    
    # Valeur par d√©faut pour min_score
    min_score = min_score or float(os.environ.get("MIN_MATCH_SCORE", "0.2"))
    
    # V√©rifier que les fichiers existent
    if not os.path.exists(jira_file):
        console.print(f"[bold red]Le fichier JIRA {jira_file} n'existe pas.[/bold red]")
        raise typer.Exit(1)
    
    if not os.path.exists(confluence_file):
        console.print(f"[bold red]Le fichier Confluence {confluence_file} n'existe pas.[/bold red]")
        raise typer.Exit(1)
    
    # Cr√©er le r√©pertoire de sortie
    ensure_dir(output_dir)
    
    # D√©finir les chemins de sortie
    matches_file = os.path.join(output_dir, "jira_confluence_matches.json")
    jira_with_matches_file = os.path.join(output_dir, "jira_with_matches.json")
    confluence_with_matches_file = os.path.join(output_dir, "confluence_with_matches.json")
    
    # Afficher les infos
    console.print(Panel.fit(
        f"[bold]Matching JIRA ‚Üî Confluence[/bold]\n\n"
        f"JIRA : [cyan]{jira_file}[/cyan]\n"
        f"Confluence : [cyan]{confluence_file}[/cyan]\n"
        f"Score minimum : [cyan]{min_score}[/cyan]",
        title="Matching",
        border_style="blue"
    ))
    
    # Construire le chemin vers le script match_jira_confluence.py
    match_script = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), 
                             "extract", "match_jira_confluence.py")
    
    # Ex√©cuter le script de matching
    cmd = [
        sys.executable, match_script,
        "--jira", jira_file,
        "--confluence", confluence_file,
        "--output", matches_file,
        "--updated-jira", jira_with_matches_file,
        "--updated-confluence", confluence_with_matches_file,
        "--min-score", str(min_score)
    ]
    
    with console.status("[bold blue]Matching en cours..."):
        from subprocess import run, PIPE
        result = run(cmd, stdout=PIPE, stderr=PIPE, text=True)
    
    if result.returncode != 0:
        console.print("[bold red]Erreur lors du matching:[/bold red]")
        console.print(result.stderr)
        raise typer.Exit(1)
    
    # Afficher les r√©sultats
    console.print(result.stdout)
    
    # LLM pour am√©liorer les correspondances si demand√©
    if llm_assist:
        # Utiliser les valeurs par d√©faut si non sp√©cifi√©es
        if not api_key:
            api_key = os.environ.get("OPENAI_API_KEY")
            
        if not llm_model:
            llm_model = os.environ.get("DEFAULT_LLM_MODEL", "gpt-4.1")
            
        if not api_key:
            console.print("[bold yellow]Pas de cl√© API OpenAI trouv√©e. L'assistance LLM ne sera pas effectu√©e.[/bold yellow]")
            llm_assist = False
        
        if llm_assist:
            console.print("[bold]Am√©lioration des correspondances avec LLM...[/bold]")
            
            # TODO: Impl√©menter l'am√©lioration des correspondances avec LLM
            # Cela pourrait inclure:
            # 1. Analyse des correspondances de faible score pour confirmer/infirmer
            # 2. Recherche de correspondances suppl√©mentaires bas√©es sur la s√©mantique
            # 3. Suggestion de nouveaux liens qui n'ont pas √©t√© d√©tect√©s automatiquement
            
            console.print("[bold yellow]Assistance LLM pour les correspondances non impl√©ment√©e.[/bold yellow]")
    
    # Afficher un r√©sum√©
    try:
        with open(matches_file, 'r', encoding='utf-8') as f:
            matches = json.load(f)
        
        table = Table(title="R√©sum√© des correspondances")
        table.add_column("M√©trique", style="cyan")
        table.add_column("Valeur", style="green")
        
        # Nombre de tickets avec correspondances
        total_tickets = len(matches)
        table.add_row("Tickets avec correspondances", str(total_tickets))
        
        # Nombre total de correspondances
        total_matches = sum(len(matches[ticket_id]) for ticket_id in matches)
        table.add_row("Correspondances totales", str(total_matches))
        
        # Moyenne de correspondances par ticket
        avg_matches = total_matches / total_tickets if total_tickets > 0 else 0
        table.add_row("Moyenne par ticket", f"{avg_matches:.2f}")
        
        console.print(table)
        
        console.print(f"\nFichiers g√©n√©r√©s dans : [bold cyan]{output_dir}[/bold cyan]")
        console.print("\n[bold green]Matching termin√© avec succ√®s ![/bold green]")
        
    except Exception as e:
        console.print(f"[bold red]Erreur lors de l'affichage du r√©sum√©: {e}[/bold red]")


@app.command()
def unified(
    jira_files: List[str] = typer.Argument(..., help="Fichiers JSON JIRA √† traiter"),
    confluence_files: List[str] = typer.Option([], "--confluence", "-c", help="Fichiers JSON Confluence √† traiter"),
    output_dir: str = typer.Option(None, "--output-dir", "-o", help="R√©pertoire de sortie"),
    min_match_score: float = typer.Option(None, "--min-score", "-s", help="Score minimum pour les correspondances"),
    max_items: Optional[int] = typer.Option(None, "--max", help="Nombre maximum d'√©l√©ments √† traiter par fichier"),
    use_llm: bool = typer.Option(True, "--llm/--no-llm", help="Utiliser un LLM pour l'enrichissement"),
    api_key: Optional[str] = typer.Option(None, "--api-key", help="Cl√© API OpenAI (si non d√©finie dans les variables d'environnement)"),
    skip_matching: bool = typer.Option(False, "--skip-matching", help="Ne pas effectuer le matching entre JIRA et Confluence"),
    language: Optional[str] = typer.Option(None, "--lang", help="Langue √† utiliser (fr/en)"),
    compress: bool = typer.Option(True, "--compress/--no-compress", help="Compresser les fichiers de sortie avec zstd et orjson (activ√© par d√©faut)"),
    compress_level: int = typer.Option(19, "--compress-level", help="Niveau de compression zstd (1-22)"),
    keep_originals: bool = typer.Option(True, "--keep-originals/--no-originals", help="Conserver les fichiers JSON originaux en plus des compress√©s"),
):
    """
    Flux unifi√©: traite JIRA + Confluence, fait le matching et pr√©pare les donn√©es pour le LLM.
    
    Cette commande ex√©cute un workflow complet pour traiter les fichiers JIRA et Confluence:
    1. Traitement des fichiers JIRA pour extraction des donn√©es cl√©s
    2. Traitement des fichiers Confluence si fournis
    3. Matching entre Confluence et JIRA (sauf si --skip-matching est activ√©)
    4. Pr√©paration des donn√©es pour enrichissement LLM
    5. Compression des fichiers (activ√©e par d√©faut)
    """
    # G√©n√©rer un timestamp pour le nom du dossier
    from datetime import datetime
    timestamp = datetime.now().strftime("%Y%m%d%H%M%S")
    
    # Obtenir le chemin absolu vers le r√©pertoire racine du projet
    project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    
    # Utiliser le dossier results comme base √† la racine du projet
    base_results_dir = os.path.join(project_root, "results")
    os.makedirs(base_results_dir, exist_ok=True)
    
    # G√©rer le r√©pertoire de sortie
    if output_dir:
        # Si on a un dossier de sortie pr√©cis, l'utiliser avec le timestamp
        full_output_dir = os.path.join(base_results_dir, f"{output_dir}_{timestamp}")
    else:
        # Cr√©er un r√©pertoire par d√©faut avec timestamp
        full_output_dir = os.path.join(base_results_dir, f"{timestamp}-unified-process")
    
    # Cr√©er le r√©pertoire
    ensure_dir(full_output_dir)
    print_info(f"R√©pertoire de sortie: {full_output_dir}")
    
    # D√©finir la valeur par d√©faut pour min_match_score si non sp√©cifi√©e
    min_score = min_match_score or float(os.environ.get("MIN_MATCH_SCORE", "0.2"))
    print_info(f"Score minimum pour les correspondances: {min_score}")
    
    # S'assurer que la langue est configur√©e
    if language:
        language_str = str(language)
        # Ne pas afficher l'objet OptionInfo, juste la valeur
        console.print(f"üåê Langue configur√©e: {language_str}")
    
    # Options pour run_unified_analysis.py
    cmd = [
        sys.executable,
        os.path.join(parent_dir, "extract", "run_unified_analysis.py"),
        "--jira-files"
    ] + jira_files
    
    if confluence_files:
        cmd.extend(["--confluence-files"] + confluence_files)
    
    cmd.extend([
        "--output-dir", str(full_output_dir),
        "--min-match-score", str(min_score)
    ])
    
    if max_items:
        cmd.extend(["--max-items", str(max_items)])
    
    if use_llm:
        cmd.append("--with-openai")
        if api_key:
            cmd.extend(["--api-key", str(api_key)])
    else:
        cmd.append("--no-openai")
    
    if skip_matching:
        cmd.append("--skip-matching")
    
    if language:
        cmd.extend(["--language", str(language)])
    
    # Toujours ajouter les options de compression
    # M√™me si compress est False dans les param√®tres, on force l'activation
    cmd.append("--compress")
    cmd.extend(["--compress-level", str(compress_level)])
    # Force keep_originals √† True
    cmd.append("--keep-originals")
    
    # Ex√©cuter le script run_unified_analysis.py
    try:
        console.print("\n[bold cyan]Ex√©cution du flux unifi√©...[/bold cyan]")
        process = subprocess.run(cmd, text=True, capture_output=True)
        
        if process.returncode == 0:
            console.print(process.stdout)
            
            # Corriger les √©ventuels probl√®mes de dossiers dupliqu√©s
            try:
                # Importer le module tools
                from tools import fix_duplicate_paths
                
                # Appliquer la correction au r√©pertoire de sortie
                moved_count = fix_duplicate_paths(full_output_dir)
                if moved_count > 0:
                    console.print(f"[yellow]‚ö†Ô∏è Correction de {moved_count} fichiers dans des chemins dupliqu√©s[/yellow]")
            except ImportError:
                console.print("[yellow]Module tools non trouv√©. Pas de correction de chemins dupliqu√©s.[/yellow]")
            except Exception as e:
                console.print(f"[yellow]Erreur lors de la correction des chemins: {str(e)}[/yellow]")
            
            print_success(f"Fichiers g√©n√©r√©s dans : {full_output_dir}")
            console.print(f"Arborescence globale g√©n√©r√©e dans : {os.path.join(full_output_dir, 'global_arborescence.txt')}")
            
            # Afficher des informations sur la compression
            current_lang = get_current_language()
            report_path = os.path.join(full_output_dir, f"compression_report_{current_lang}.txt")
            if os.path.exists(report_path):
                console.print(f"[bold cyan]Rapport de compression g√©n√©r√© dans : [/bold cyan]{report_path}")
            
            print_success("Traitement unifi√© termin√© avec succ√®s !")
        else:
            console.print(process.stdout)
            console.print(process.stderr)
            print_error("Le traitement a √©chou√©. Consultez les messages d'erreur ci-dessus.")
            
    except Exception as e:
        print_error(f"Erreur lors de l'ex√©cution du flux unifi√©: {str(e)}")
        return


# Ajouter un nouveau d√©corateur app.callback pour g√©rer l'option de langue globale
@app.callback()
def main(
    language: str = typer.Option(None, "--lang", "-l", help="Langue de l'interface (fr/en)")
):
    """
    JSON Processor pour Llamendex - Outil d'analyse et de transformation de fichiers JSON.
    """
    if language:
        set_language(language)


def _run_interactive_match():
    """Interface interactive pour la commande match."""
    # S√©lection des fichiers
    console.print("[bold]S√©lection des fichiers JIRA et Confluence[/bold]")
    
    jira_file = _prompt_for_file("S√©lectionnez le fichier JIRA trait√©:")
    if not jira_file:
        return
    
    confluence_file = _prompt_for_file("S√©lectionnez le fichier Confluence trait√©:")
    if not confluence_file:
        return
    
    # R√©pertoire de sortie
    default_output_dir = os.environ.get("MATCH_OUTPUT_DIR", "output_matches")
    output_dir = typer.prompt("R√©pertoire de sortie", default=default_output_dir)
    
    # Options de matching
    questions = [
        inquirer.Text('min_score',
                     message="Score minimum pour les correspondances (0.0-1.0)",
                     default="0.2"),
        inquirer.Confirm('llm_assist',
                        message="Utiliser un LLM pour am√©liorer les correspondances ?",
                        default=False),
        inquirer.Confirm('compress',
                        message="Activer la compression des fichiers JSON ?",
                        default=False)
    ]
    answers = inquirer.prompt(questions)
    
    min_score = float(answers['min_score'])
    llm_assist = answers['llm_assist']
    compress = answers['compress']
    
    # Options de compression si activ√©e
    compress_level = 19  # Valeur par d√©faut
    keep_originals = True  # Valeur par d√©faut
    if compress:
        compression_questions = [
            inquirer.List('compress_level',
                          message=t("compression_level_prompt", "compression"),
                          choices=["15 (fast)", "19 (balanced)", "22 (max)"],
                          default="19 (balanced)"),
            inquirer.Confirm('keep_originals',
                             message=t("keep_originals_prompt", "compression"),
                             default=True)
        ]
        compression_answers = inquirer.prompt(compression_questions)
        compress_level = int(compression_answers['compress_level'].split(" ")[0])
        keep_originals = compression_answers['keep_originals']
    
    # Options LLM si demand√©es
    llm_model = None
    api_key = None
    if llm_assist:
        # Cl√© API
        api_key = os.environ.get("OPENAI_API_KEY")
        if not api_key:
            api_key = typer.prompt("Entrez votre cl√© API OpenAI", hide_input=True)
        
        # Mod√®le
        questions = [
            inquirer.List('llm_model',
                         message="Choisissez un mod√®le LLM",
                         choices=LLM_MODELS,
                         default="gpt-4.1")
        ]
        llm_answers = inquirer.prompt(questions)
        llm_model = llm_answers['llm_model']
    
    # Confirmation finale
    console.print("\n[bold]R√©capitulatif :[/bold]")
    console.print(f"- Fichier JIRA : [cyan]{jira_file}[/cyan]")
    console.print(f"- Fichier Confluence : [cyan]{confluence_file}[/cyan]")
    console.print(f"- R√©pertoire de sortie : [cyan]{output_dir}[/cyan]")
    console.print(f"- Score minimum : [cyan]{min_score}[/cyan]")
    if llm_assist:
        console.print(f"- Assistance LLM : [cyan]Oui ({llm_model})[/cyan]")
    if compress:
        console.print(f"- Compression JSON : [cyan]Oui (niveau {compress_level}, conserver originaux: {t('yes', 'messages') if keep_originals else t('no', 'messages')})[/cyan]")
    else:
        console.print(f"- Compression JSON : [cyan]Non[/cyan]")
    
    questions = [
        inquirer.Confirm('confirm',
                        message="Lancer le matching ?",
                        default=True)
    ]
    confirm = inquirer.prompt(questions)
    
    if not confirm or not confirm['confirm']:
        console.print("[yellow]Op√©ration annul√©e.[/yellow]")
        return
    
    # Ex√©cuter match avec les param√®tres d√©finis
    match(
        jira_file=jira_file,
        confluence_file=confluence_file,
        output_dir=output_dir,
        min_score=min_score,
        llm_assist=llm_assist,
        api_key=api_key,
        llm_model=llm_model,
        compress=compress,
        compress_level=compress_level if compress else None,
        keep_originals=keep_originals if compress else None
    )


def _run_interactive_unified():
    """Interface interactive pour la commande unified."""
    console.print("[bold]Flux complet: JIRA + Confluence + Matching[/bold]")
    
    # S√©lection des fichiers JIRA (multiples)
    jira_files = []
    while True:
        jira_file = _prompt_for_file(f"S√©lectionnez un fichier JIRA ({len(jira_files)} s√©lectionn√©s, [Valider la s√©lection] pour terminer):", allow_validate=True)
        if not jira_file or jira_file == "__VALIDATE__":
            break
        jira_files.append(jira_file)
        console.print(f"Fichier ajout√© : [cyan]{jira_file}[/cyan]")
    
    if not jira_files:
        console.print("[yellow]Aucun fichier JIRA s√©lectionn√©. Op√©ration annul√©e.[/yellow]")
        return
    
    # S√©lection des fichiers Confluence (facultatif)
    confluence_files = []
    questions = [
        inquirer.Confirm('add_confluence', message="Ajouter des fichiers Confluence ?", default=True)
    ]
    add_conf = inquirer.prompt(questions)
    if add_conf and add_conf['add_confluence']:
        while True:
            confluence_file = _prompt_for_file(f"S√©lectionnez un fichier Confluence ({len(confluence_files)} s√©lectionn√©s, [Valider la s√©lection] pour terminer):", allow_validate=True)
            if not confluence_file or confluence_file == "__VALIDATE__":
                break
            confluence_files.append(confluence_file)
            console.print(f"Fichier ajout√© : [cyan]{confluence_file}[/cyan]")
    
    # R√©pertoire de sortie - Utiliser un input simple plut√¥t que typer.prompt pour √©viter les probl√®mes d'objets OptionInfo
    default_output_dir = os.environ.get("UNIFIED_OUTPUT_DIR", "output_unified")
    output_dir_str = input(f"R√©pertoire de sortie [{default_output_dir}]: ").strip()
    output_dir_str = output_dir_str if output_dir_str else default_output_dir
    
    # Options avanc√©es
    questions = [
        inquirer.Text('min_match_score',
                     message="Score minimum pour les correspondances (0.0-1.0)",
                     default="0.2"),
        inquirer.Text('max_items',
                     message="Nombre maximum d'√©l√©ments √† traiter par fichier (vide = tous)",
                     default=""),
        inquirer.Confirm('use_llm',
                        message="Utiliser un LLM pour l'enrichissement ?",
                        default=False),
        inquirer.Confirm('skip_matching',
                        message="Ignorer le matching entre JIRA et Confluence ?",
                        default=False),
        inquirer.Confirm('compress',
                        message="Activer la compression des fichiers JSON ?",
                        default=False)
    ]
    answers = inquirer.prompt(questions)
    
    min_match_score = float(answers['min_match_score'])
    max_items = int(answers['max_items']) if answers['max_items'].strip() else None
    use_llm = answers['use_llm']
    skip_matching = answers['skip_matching']
    compress = answers['compress']
    
    # Options de compression si activ√©e
    compress_level = 19  # Valeur par d√©faut
    keep_originals = True  # Valeur par d√©faut
    if compress:
        compression_questions = [
            inquirer.List('compress_level',
                          message=t("compression_level_prompt", "compression"),
                          choices=["15 (fast)", "19 (balanced)", "22 (max)"],
                          default="19 (balanced)"),
            inquirer.Confirm('keep_originals',
                             message=t("keep_originals_prompt", "compression"),
                             default=True)
        ]
        compression_answers = inquirer.prompt(compression_questions)
        compress_level = int(compression_answers['compress_level'].split(" ")[0])
        keep_originals = compression_answers['keep_originals']
    
    # Options LLM si demand√©es
    api_key = None
    if use_llm:
        # Cl√© API
        api_key = os.environ.get("OPENAI_API_KEY")
        if not api_key:
            api_key = input("Entrez votre cl√© API OpenAI: ").strip()
    
    # Confirmation finale
    console.print("\n[bold]R√©capitulatif :[/bold]")
    console.print(f"- Fichiers JIRA : [cyan]{', '.join(jira_files)}[/cyan]")
    if confluence_files:
        console.print(f"- Fichiers Confluence : [cyan]{', '.join(confluence_files)}[/cyan]")
    console.print(f"- R√©pertoire de sortie : [cyan]{output_dir_str}[/cyan]")
    console.print(f"- Score minimum : [cyan]{min_match_score}[/cyan]")
    if max_items:
        console.print(f"- Limite d'√©l√©ments : [cyan]{max_items}[/cyan]")
    console.print(f"- Enrichissement LLM : [cyan]{'Oui' if use_llm else 'Non'}[/cyan]")
    console.print(f"- Ignorer matching : [cyan]{'Oui' if skip_matching else 'Non'}[/cyan]")
    if compress:
        console.print(f"- Compression JSON : [cyan]Oui (niveau {compress_level}, conserver originaux: {t('yes', 'messages') if keep_originals else t('no', 'messages')})[/cyan]")
    else:
        console.print(f"- Compression JSON : [cyan]Non[/cyan]")
    
    questions = [
        inquirer.Confirm('confirm',
                        message="Lancer le traitement unifi√© ?",
                        default=True)
    ]
    confirm = inquirer.prompt(questions)
    
    if not confirm or not confirm['confirm']:
        console.print("[yellow]Op√©ration annul√©e.[/yellow]")
        return
    
    # Ex√©cuter unified avec les param√®tres d√©finis
    unified(
        jira_files=jira_files,
        confluence_files=confluence_files,
        output_dir=output_dir_str,  # Passer explicitement une cha√Æne de caract√®res
        min_match_score=min_match_score,
        max_items=max_items,
        use_llm=use_llm,
        api_key=api_key,
        skip_matching=skip_matching,
        compress=compress,
        compress_level=compress_level if compress else None,
        keep_originals=keep_originals if compress else None
    )


@app.command()
def describe_images(
    input_file: str = typer.Argument(..., help="Fichier PDF √† analyser (doit se trouver dans le dossier files/)"),
    output_file: Optional[str] = typer.Option(None, "--output", "-o", help="Fichier de sortie JSON (par d√©faut: dans /results/{pdf_base}_{timestamp}/...)"),
    max_images: int = typer.Option(10, "--max-images", help="Nombre maximum d'images √† traiter"),
    timeout: int = typer.Option(30, "--timeout", help="Timeout en secondes pour l'appel API OpenAI"),
    language: str = typer.Option(get_current_language(), "--lang", "-l", help="Langue de description (fr, en)"),
    api_key: Optional[str] = typer.Option(None, "--api-key", help="Cl√© API OpenAI (par d√©faut: variable d'environnement OPENAI_API_KEY)"),
    model: Optional[str] = typer.Option(None, "--model", help="Mod√®le OpenAI multimodal (par d√©faut: variable d'environnement VISION_LLM_MODEL)"),
    no_save_images: bool = typer.Option(False, "--no-save-images", help="Ne pas sauvegarder les images en fichiers PNG"),
    compress: bool = typer.Option(False, "--compress", help="Compresser le fichier de sortie avec zstd et orjson"),
    compress_level: int = typer.Option(19, "--compress-level", help="Niveau de compression (1-22, 19 par d√©faut)"),
    keep_originals: bool = typer.Option(True, "--keep-originals/--no-originals", help="Conserver les fichiers JSON originaux apr√®s compression"),
):
    """
    Analyse un fichier PDF pour extraire et d√©crire les images avec IA multimodale.
    Ajoute une description IA pour chaque image d√©tect√©e et g√©n√®re un fichier JSON.
    """
    try:
        console = rich.console.Console()
        
        # V√©rifier que le fichier existe
        if not os.path.exists(input_file):
            # Tenter de trouver le fichier dans le dossier files/
            files_dir = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), "files")
            potential_file = os.path.join(files_dir, input_file)
            if os.path.exists(potential_file):
                input_file = potential_file
            else:
                console.print(f"[bold red]Erreur[/bold red]: Le fichier {input_file} n'existe pas.")
                return
        
        # R√©cup√©rer la cl√© API OpenAI
        if not api_key:
            api_key = os.environ.get("OPENAI_API_KEY")
            if not api_key:
                console.print("[bold red]Erreur[/bold red]: Aucune cl√© API OpenAI fournie.")
                console.print("D√©finissez la variable d'environnement OPENAI_API_KEY, ajoutez-la dans .env, ou utilisez --api-key.")
                return
        
        # Configuration du dossier de sortie
        pdf_base = os.path.splitext(os.path.basename(input_file))[0]
        timestamp = time.strftime("%Y-%m-%d-%H-%M-%S")
        output_dir_name = f"{pdf_base}_{timestamp}"
        
        # Cr√©er le dossier de sortie dans /results/ (pas dans /results/results/)
        results_dir = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), "results")
        output_dir = os.path.join(results_dir, output_dir_name)
        
        os.makedirs(output_dir, exist_ok=True)
        
        # D√©finir le nom du fichier de sortie
        if not output_file:
            output_name = f"{pdf_base}_images_described.json"
            output_file = os.path.join(output_dir, output_name)
        elif not os.path.isabs(output_file):
            output_file = os.path.join(output_dir, output_file)
        
        # Initialiser le module d'extraction et de description
        from extract.image_describer import PDFImageDescriber
        
        # Afficher un message de d√©marrage
        console.print(f"\n[bold green]Analyse du document PDF[/bold green]: {input_file}")
        console.print(f"[bold]Langue[/bold]: {language}")
        console.print(f"[bold]Mod√®le[/bold]: {model or os.environ.get('VISION_LLM_MODEL', 'gpt-4o')}")
        console.print(f"[bold]Images max[/bold]: {max_images}")
        console.print(f"[bold]Sauvegarde des images[/bold]: {'Non' if no_save_images else 'Oui'}")
        
        with console.status("[bold green]Analyse en cours...[/bold green]"):
            # Extraire et d√©crire les images
            describer = PDFImageDescriber(
                openai_api_key=api_key,
                max_images=max_images,
                timeout=timeout,
                language=language,
                model=model,
                save_images=not no_save_images
            )
            
            # Utiliser le r√©pertoire de sortie cr√©√©
            result = describer.describe_images_in_pdf(input_file, output_dir)
        
        # Afficher un r√©sum√©
        console.print(f"\n[bold green]R√©sultats[/bold green]:")
        console.print(f"  ‚Ä¢ Images d√©tect√©es: [bold]{result['nb_images_detectees']}[/bold]")
        console.print(f"  ‚Ä¢ Images analys√©es: [bold]{result['nb_images_analysees']}[/bold]")
        console.print(f"  ‚Ä¢ R√©pertoire de sortie: [bold]{output_dir}[/bold]")
        
        # Chercher les fichiers g√©n√©r√©s
        json_files = glob.glob(os.path.join(output_dir, "*.json"))
        image_files = glob.glob(os.path.join(output_dir, "*.png"))
        txt_files = glob.glob(os.path.join(output_dir, "*.txt"))
        
        console.print(f"  ‚Ä¢ Fichiers JSON: [bold]{len(json_files)}[/bold]")
        if not no_save_images:
            console.print(f"  ‚Ä¢ Fichiers image: [bold]{len(image_files)}[/bold]")
        console.print(f"  ‚Ä¢ Fichiers texte: [bold]{len(txt_files)}[/bold]")
        
        # Si un rapport texte a √©t√© g√©n√©r√©, l'afficher
        report_files = [f for f in txt_files if "_report.txt" in f]
        if report_files:
            console.print(f"\n[bold green]Un rapport d√©taill√© a √©t√© g√©n√©r√©:[/bold green] {report_files[0]}")
        
        # Compresser le r√©sultat si demand√©
        if compress:
            console.print("\n[bold]Compression des r√©sultats...[/bold]")
            
            from extract.compress_utils import compress_results_directory
            compression_result = compress_results_directory(
                output_dir,
                compress_level=compress_level,
                keep_originals=keep_originals,
                language=language
            )
            
            console.print(f"[bold green]Compression termin√©e[/bold green]: {compression_result['files_stats']['nb_files_compressed']} fichiers compress√©s")
            console.print(f"Rapport de compression: [bold]{compression_result['report_path']}[/bold]")
        
        return output_dir
    
    except Exception as e:
        console = rich.console.Console()
        console.print(f"[bold red]Erreur lors du traitement[/bold red]: {str(e)}")
        import traceback
        console.print(traceback.format_exc())
        return None

def _run_interactive_describe_images():
    """Interface interactive pour la commande describe-images."""
    input_path = _prompt_for_file(t("select_file_pdf", "prompts"), file_extension=".pdf")
    if not input_path:
        return
    
    # D√©terminer la langue actuelle
    language = get_current_language()
    
    questions = [
        inquirer.Text('output', message=t("output_file", "prompts"), default=""),
        inquirer.Text('max_images', message="Nombre max d'images √† traiter (d√©faut: 10)", default="10"),
        inquirer.Text('timeout', message="Timeout API en secondes (d√©faut: 30)", default="30"),
        inquirer.Confirm('save_images', message="Sauvegarder les images extraites en PNG?", default=True),
        inquirer.Confirm('compress', message="Compresser le fichier de sortie?", default=False),
    ]
    
    if inquirer.prompt(questions)['compress']:
        questions_compress = [
            inquirer.Text('compress_level', message="Niveau de compression (1-22, d√©faut: 19)", default="19"),
            inquirer.Confirm('keep_originals', message="Conserver les fichiers originaux?", default=True),
        ]
        compress_answers = inquirer.prompt(questions_compress)
    else:
        compress_answers = {'compress_level': '19', 'keep_originals': True}
    
    answers = inquirer.prompt(questions)
    
    # R√©cup√©rer la cl√© API depuis l'environnement
    api_key = os.environ.get("OPENAI_API_KEY")
    
    # Obtenir le mod√®le depuis l'environnement
    model = os.environ.get("VISION_LLM_MODEL")
    
    # Si la cl√© API n'est pas d√©finie, la demander √† l'utilisateur
    if not api_key:
        api_key_question = [
            inquirer.Text('api_key', message="Enter your OpenAI API key: ")
        ]
        api_key = inquirer.prompt(api_key_question)['api_key']
    
    # Construire les options pour l'appel de commande
    options = {}
    
    if answers['output']:
        options['output_file'] = answers['output']
    
    try:
        options['max_images'] = int(answers['max_images'])
    except ValueError:
        options['max_images'] = 10
    
    try:
        options['timeout'] = int(answers['timeout'])
    except ValueError:
        options['timeout'] = 30
    
    options['language'] = language
    options['api_key'] = api_key
    options['model'] = model
    options['no_save_images'] = not answers['save_images']
    options['compress'] = answers['compress']
    
    if answers['compress']:
        try:
            options['compress_level'] = int(compress_answers['compress_level'])
        except ValueError:
            options['compress_level'] = 19
        options['keep_originals'] = compress_answers['keep_originals']
    
    # Appeler la commande avec les options
    describe_images(input_path, **options)


# Cr√©er un groupe de commandes pour l'extraction d'images
extract_images = typer.Typer(help="Commandes pour l'extraction et l'analyse d'images depuis des PDF")
app.add_typer(extract_images, name="extract-images", help="Extraction et analyse d'images depuis des PDF")

@extract_images.command(name="describe", help="Extraire et d√©crire les images d'un fichier PDF avec IA multimodale")
def extract_pdf_images(
    pdf_path: str = typer.Argument(..., help="Fichier PDF √† analyser"),
    max_images: int = typer.Option(10, "--max-images", "-m", help="Nombre maximum d'images √† traiter"),
    timeout: int = typer.Option(30, "--timeout", "-t", help="Timeout pour l'appel API (secondes)"),
    language: str = typer.Option("fr", "--language", "-l", help="Langue de description (fr, en)"),
    output: Optional[str] = typer.Option(None, "--output", "-o", help="R√©pertoire de sortie pour les r√©sultats"),
    no_save_images: bool = typer.Option(False, "--no-save-images", help="Ne pas sauvegarder les images en fichiers PNG"),
    model: Optional[str] = typer.Option(None, "--model", help="Mod√®le OpenAI √† utiliser (par d√©faut: VISION_LLM_MODEL)")
):
    """Extraire et d√©crire les images d'un fichier PDF avec une IA multimodale"""
    from extract.image_describer import PDFImageDescriber
    import os
    from dotenv import load_dotenv
    
    load_dotenv()
    
    # V√©rifier que le fichier existe
    if not os.path.exists(pdf_path):
        # Tenter de trouver le fichier dans le dossier files/
        files_dir = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), "files")
        potential_file = os.path.join(files_dir, pdf_path)
        if os.path.exists(potential_file):
            pdf_path = potential_file
        else:
            console.print(f"[bold red]Erreur[/bold red]: Le fichier {pdf_path} n'existe pas.")
            return
    
    # R√©cup√©rer la cl√© API depuis l'environnement
    api_key = os.environ.get("OPENAI_API_KEY")
    if not api_key:
        console.print("[bold red]Erreur: La cl√© API OpenAI n'est pas d√©finie.[/bold red]")
        console.print("D√©finissez la variable d'environnement OPENAI_API_KEY ou ajoutez-la dans le fichier .env")
        return
    
    # R√©cup√©rer le mod√®le depuis l'environnement ou utiliser la valeur par d√©faut
    # Priorit√©: param√®tre mod√®le > variable VISION_LLM_MODEL > valeur par d√©faut gpt-4o
    vision_model = model or os.environ.get("VISION_LLM_MODEL") or "gpt-4o"
    
    console.print(f"[bold]Analyse du PDF[/bold]: {pdf_path}")
    console.print(f"[dim]Param√®tres: max_images={max_images}, timeout={timeout}, language={language}, mod√®le={vision_model}[/dim]")
    
    # Initialiser le descripteur d'images
    describer = PDFImageDescriber(
        openai_api_key=api_key,
        max_images=max_images,
        timeout=timeout,
        language=language,
        model=vision_model,
        save_images=not no_save_images,
    )
    
    # Analyser le document
    result = describer.describe_images_in_pdf(pdf_path, output)
    
    # Afficher un r√©sum√© des r√©sultats
    console.print(f"\n[bold]R√©sum√© de l'analyse:[/bold]")
    console.print(f"  Fichier: {result['meta']['filename']}")
    console.print(f"  Images d√©tect√©es: {result['nb_images_detectees']}")
    console.print(f"  Images analys√©es: {result['nb_images_analysees']}")
    console.print(f"  R√©sultats sauvegard√©s dans: {result['meta']['output_dir']}")

@extract_images.command(name="complete", help="Extraction compl√®te d'un PDF: texte et images avec analyse IA")
def extract_pdf_complete(
    pdf_path: str = typer.Argument(..., help="Fichier PDF √† analyser"),
    max_images: int = typer.Option(10, "--max-images", "-m", help="Nombre maximum d'images √† traiter"),
    timeout: int = typer.Option(30, "--timeout", "-t", help="Timeout pour l'appel API (secondes)"),
    language: str = typer.Option("fr", "--language", "-l", help="Langue de description (fr, en)"),
    output: Optional[str] = typer.Option(None, "--output", "-o", help="R√©pertoire de sortie pour les r√©sultats"),
    no_save_images: bool = typer.Option(False, "--no-save-images", help="Ne pas sauvegarder les images en fichiers PNG"),
    model: Optional[str] = typer.Option(None, "--model", help="Mod√®le OpenAI √† utiliser (par d√©faut: VISION_LLM_MODEL)"),
    create_zip: bool = typer.Option(False, "--create-zip", "-z", help="Cr√©er un fichier ZIP avec tous les r√©sultats")
):
    """
    Extrait le texte et les images d'un PDF. Analyse les images avec IA et g√©n√®re un JSON unifi√©.
    Peut √©galement cr√©er un fichier ZIP contenant tous les r√©sultats si --create-zip est sp√©cifi√©.
    """
    from extract.pdf_complete_extractor import PDFCompleteExtractor
    import os
    from dotenv import load_dotenv
    
    load_dotenv()
    
    # V√©rifier que le fichier existe
    if not os.path.exists(pdf_path):
        # Tenter de trouver le fichier dans le dossier files/
        files_dir = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), "files")
        potential_file = os.path.join(files_dir, pdf_path)
        if os.path.exists(potential_file):
            pdf_path = potential_file
        else:
            console.print(f"[bold red]Erreur[/bold red]: Le fichier {pdf_path} n'existe pas.")
            return
    
    # R√©cup√©rer la cl√© API depuis l'environnement
    api_key = os.environ.get("OPENAI_API_KEY")
    if not api_key:
        console.print("[bold red]Erreur: La cl√© API OpenAI n'est pas d√©finie.[/bold red]")
        console.print("D√©finissez la variable d'environnement OPENAI_API_KEY ou ajoutez-la dans le fichier .env")
        return
    
    # R√©cup√©rer le mod√®le depuis l'environnement ou utiliser la valeur par d√©faut
    # Priorit√©: param√®tre mod√®le > variable VISION_LLM_MODEL > valeur par d√©faut gpt-4o
    vision_model = model or os.environ.get("VISION_LLM_MODEL") or "gpt-4o"
    
    console.print(f"[bold]Extraction compl√®te du PDF[/bold]: {pdf_path}")
    console.print(f"[dim]Param√®tres: max_images={max_images}, timeout={timeout}, language={language}, mod√®le={vision_model}, create_zip={create_zip}[/dim]")
    
    # Initialiser l'extracteur complet
    extractor = PDFCompleteExtractor(
        openai_api_key=api_key,
        max_images=max_images,
        timeout=timeout,
        language=language,
        model=vision_model,
        save_images=not no_save_images,
    )
    
    # Analyser le document
    result = extractor.process_pdf(pdf_path, output)
    
    # Afficher un r√©sum√© des r√©sultats
    console.print(f"\n[bold]R√©sum√© de l'extraction compl√®te:[/bold]")
    console.print(f"  Fichier: {result['meta']['filename']}")
    console.print(f"  Pages extraites: {len(result['pages'])}")
    console.print(f"  Images d√©tect√©es: {result.get('nb_images_detectees', 0)}")
    console.print(f"  Images analys√©es: {result.get('nb_images_analysees', 0)}")
    console.print(f"  R√©sultats sauvegard√©s dans: {result['meta']['output_dir']}")
    
    # Cr√©er un ZIP si demand√©
    if create_zip:
        import zipfile
        from datetime import datetime
        
        # Chemin du dossier de sortie
        output_dir = result['meta']['output_dir']
        
        # Nom du fichier ZIP bas√© sur le nom du PDF et timestamp
        pdf_name = os.path.splitext(os.path.basename(pdf_path))[0]
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        zip_filename = f"{pdf_name}_complete_results_{timestamp}.zip"
        
        # Chemin du ZIP dans le m√™me dossier que le dossier de sortie
        zip_path = os.path.join(os.path.dirname(output_dir), zip_filename)
        
        console.print(f"\n[bold]Cr√©ation du fichier ZIP[/bold]")
        
        # Cr√©er le ZIP
        with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zip_file:
            files_added = 0
            for file_path in Path(output_dir).glob('**/*'):
                if file_path.is_file():
                    # Utiliser un chemin relatif dans l'archive
                    arcname = file_path.relative_to(output_dir)
                    zip_file.write(file_path, arcname=str(arcname))
                    files_added += 1
        
        console.print(f"  Fichier ZIP cr√©√©: [cyan]{zip_path}[/cyan]")
        console.print(f"  Fichiers ajout√©s: {files_added}")
        
        return {
            **result,
            "zip_file": zip_path
        }

def _run_interactive_extract_complete():
    """Interface interactive pour la commande extract-images complete."""
    input_path = _prompt_for_file(t("select_file_pdf", "prompts"), file_extension=".pdf")
    if not input_path:
        return
    
    # D√©terminer la langue actuelle
    language = get_current_language()
    
    questions = [
        inquirer.Text('output', message=t("output_dir", "prompts"), default=""),
        inquirer.Text('max_images', message="Nombre max d'images √† traiter (d√©faut: 10)", default="10"),
        inquirer.Text('timeout', message="Timeout API en secondes (d√©faut: 30)", default="30"),
        inquirer.Confirm('save_images', message="Sauvegarder les images extraites en PNG?", default=True),
    ]
    
    answers = inquirer.prompt(questions)
    
    # R√©cup√©rer la cl√© API depuis l'environnement
    api_key = os.environ.get("OPENAI_API_KEY")
    
    # Obtenir le mod√®le depuis l'environnement
    model = os.environ.get("VISION_LLM_MODEL")
    
    # Si la cl√© API n'est pas d√©finie, la demander √† l'utilisateur
    if not api_key:
        api_key_question = [
            inquirer.Text('api_key', message="Enter your OpenAI API key: ")
        ]
        api_key = inquirer.prompt(api_key_question)['api_key']
    
    # Construire les options pour l'appel de commande
    options = {}
    
    if answers['output']:
        options['output'] = answers['output']
    
    try:
        options['max_images'] = int(answers['max_images'])
    except ValueError:
        options['max_images'] = 10
    
    try:
        options['timeout'] = int(answers['timeout'])
    except ValueError:
        options['timeout'] = 30
    
    options['language'] = language
    options['no_save_images'] = not answers['save_images']
    options['model'] = model
    
    # Appeler la commande avec les options
    extract_pdf_complete(input_path, **options)


if __name__ == "__main__":
    print_header()
    # S'assurer que les d√©pendances sont install√©es
    try:
        import inquirer
        import dotenv
    except ImportError:
        console.print("[yellow]Installation des d√©pendances requises...[/yellow]")
        from subprocess import run
        deps = ["inquirer", "python-dotenv", "typer", "rich"]
        run([sys.executable, "-m", "pip", "install"] + deps, check=True)
        import inquirer
        import dotenv
    
    app() 